namespace rr
{

/**

\file doc
\brief Documentation only.


\mainpage Lightsprint SDK

 \image html intro.jpg

 \section root_welcome Welcome to Lightsprint Software Development Kit
 - \subpage main_introduction
 - \subpage main_news
 - \subpage main_scenarios
 - \subpage main_features
 - \subpage main_platforms
 - \subpage main_credits

 \section root_guide Programming guide
 - \subpage main_ue3
 - \subpage main_integration
 - \subpage main_inputs
 - \subpage main_data_access
 - \subpage main_conventions "Conventions (units, scale, floats...)"
 - \subpage main_api

 \section root_samples Samples and tutorials
 - \subpage main_samples




\page main_introduction Introduction

  Realistic illumination looks good and attracts more users.
  While direct illumination has been 'solved' by science and software developers of 20th century,
  global illumination still challenges both sides. \subpage details

  Lightsprint is first to come with physically correct 
  global illumination synthesis so fast, that it 
  is suitable not only for precalculations, but also for realtime rendering in dynamic scenes.

  Lightsprint SDK offers you single flexible API for both realtime global illumination rendering
  and non-realtime lighting precalculations.

 \image html lowpoly3.jpg
  <center>realtime global illumination in scene with dynamic light and dynamic objects</center>


\page details Details
 In our real world, we see visible light coming mostly from special surfaces 
 (hot wolfram fibre in bulb, luminofor of fluorescent lamp)
 or from whole volumes of plasma (sun, fire).
 We call these surfaces and volumes <b>source of direct illumination</b>.

 Light from sources of direct illumination reaches other surfaces 
 where part of light gets absorbed and part reflected.
 Reflected part is what we see and what makes objects look lit by direct illumination.
 When we see the reflected part, we say that object has <b>direct illumination</b>.

 Reflected light from direct illumination reaches other surfaces, partially reflects,
 reaches other surfaces, partially reflects etc.
 When we see sum of these reflected parts, we say that object has <b>indirect illumination</b>.

 Illumination we see in real world is sum of direct and indirect illumination.

 Computer graphics tries to simulate this process in order to generate realistic images.
 This is however very time consuming process.
 So realtime computer graphics in 99% of cases resigns to real-world sources of direct 
 illumination and uses imaginary <b>"point", "spot" or "directional" lights</b>.
 These fictitious sources of direct illumination
 allow realtime computer graphics to calculate direct illumination very quickly.
 This process includes calculation of <b>shadows in direct illumination</b>.
 There are many realtime techniques for calculating shadows in direct illumination,
 most notably shadow mapping, volumetric/stencil shadows and projected/texture based shadows.

 There is still problem with indirect illumination, which remains
 hard to be computed quickly.




\page main_scenarios Usage scenarios

 \section scenario2 Lightsprint in toolchain, precomputes GI, realtime GI previews

  - Saves months of work to graphics artists and designers,
    previously spent in lightmap rebuilds or fake light placement.

  - Realtime GI preview allows artists to find visually more attractive positions / settings for lights.

  - Saves months spent in adapting your data for external global illumination
    tools. Other tools don't support arbitrary materials and lights.

  - Fast path for incorporating realtime global illumination into future games.

 \section scenario1 Lightsprint in game or architectural visualization, renders realtime GI

  - Greatly improves realism and visual appeal.

  - Requires no level preprocessing, saves time previously spent on infinite levels builds.

  - Simplifies integration of mods and other user provided assets.

  - Supports arbitrary materials / shaders, even those writen by modders
    after game release.

  - Publicity, be first with this level of realism never seen before.

  - See <a href="http://dee.cz/rrb/RRBugs.rar">Realtime Radiosity Bugs</a>
    as an example. It's not a real game, but it shows innovative use 
    of realtime radiosity in game. It doesn't have to be just eye candy.
    
 \image html rrbugs_hint.jpg




\page main_features Features

This is a brief list of Lightsprint features, see \ref main_data_access,
\ref main_api and \ref main_samples for more details.

Global illumination
- realtime global illumination
- realtime penumbra shadows, soft shadows
- realtime color bleeding
- dynamic lights
- dynamic objects
- multithreaded, all cores/CPUs and GPU work at once
- supports work distributed in cluster of computers
- computed and rendered in HDR
- custom scale on inputs/outputs (HDR/sRGB/other)
- scene size not limited
- mesh instancing
- mesh filters, optimizers
- adaptive subdivision

Light source formats (inputs)
- spot light
- point light
- directinal light
- skybox/cubemap
- skybox/programmable
- emissive material
- area light
- linear light
- custom programmable light

Lighting computed (outputs)
- lightmap, directional lightmap
- ambient map
- ambient occlusion, global ambient occlusion
- diffuse environment map
- specular environment map
- bent normal map
- vertex buffer with colors or bent normals
- illumination of triangle, including adaptively subdivided
- illumination of vertex, including adaptively subdivided
- illumination at ray end
- any combination of direct/indirect/global illumination

Renderers
- integrates with external renderers
- contains OpenGL 2.0 shader based renderer

Scene formats
- Collada 1.4 (.DAE)
- 3DS Max (.3DS)
- Quake 3 (.BSP)
- framework for custom formats
- complete source code

Texture formats
- JPG, PNG, DDS, GIF, TGA, BMP, TIF, HDR...
- including cube textures
- including 96bit float colors

Materials
- supports all types of materials
- diffuse maps, specular maps
- normal maps, emissive maps
- water with waves and reflection...

Realtime/precomputed
- supports realtime illumination
- supports precomputed illumination
- supports mix of realtime and precomputed illumination

GPU/API access
- full control over GPU access, source code
- full control over filesystem access, source code
- full control over scene data loading, source code

Ray-Mesh collisions
- up to 200x faster than commercial physical engines
- small memory footprint, typically 10x smaller than commercial physical engines
- multithreaded
- up to 4294967295 vertices in mesh
- up to 1073741824 triangles in mesh
- triangle lists/strips/indexed/nonindexed
- floats, doubles, halfs, ints, shorts
- custom mesh data structures without data duplication
- uniform scaling, non uniform scaling
- singlesided tests, doublesided tests
- number of sides defined by material
- return one or gather all collisions
- custom action at collision
- returns intersection distance, 2D and 3D position
- returns normal, plane and face side that was hit
- high precision, higher than commercial physical engines
- high reliability, 7 years under heavy load





\page main_credits Additional credits

Lightsprint SDK uses free open source libraries listed below.
They are included here as .dll files.
You are free to download their sources and rebuild, use them in commercial applications etc.

Used by LightsprintCore:
- <a href="http://freeimage.sourceforge.net">FreeImage</a> This software uses the FreeImage open source image library.
  FreeImage is used under the FIPL, version 1.0.
  FreeImage internally uses other free open source libraries.

Used by LightsprintGL:
- <a href="http://glew.sourceforge.net/">GLEW</a>

Used by samples:
- <a href="http://www.feelingsoftware.com/content/view/62/76">FCollada</a> by Feeling Software is used under MIT license.
  FCollada internally uses other free open source libraries.
- <a href="http://www.opengl.org/resources/libraries/glut/">GLUT</a>



\page main_integration Integration

 \section integr_tool Precalculations / for toolchain
  - \subpage integration_tool_1
  - \subpage integration_tool_2

 \section integr_rt Realtime / in game
  - \subpage integration_realtime_1
  - \subpage integration_realtime_3
  - \subpage integration_realtime_4




\page integration_tool_1 Precalculations in your scene loaded from disk

 You can immediately calculate lightmaps, ambient maps and bent normals
 if you convert your scenes to supported formats and use existing
 sample applications.

 <table width=100% border=0 cellpadding=3 cellspacing=0><tr align=left style="background-color:#dddddd"><td>
 Recommended approach:
 </td></tr></table>

  - Convert your scene to Collada format,
    and load it into CPULightmaps, AmbientOcclusion or Lightmaps sample.

 <table width=100% border=0 cellpadding=3 cellspacing=0><tr align=left style="background-color:#dddddd"><td>
 Scheme of application:
 </td></tr></table>

  - CPULightmaps sample 
    is built on top of purely numerical LightsprintCore and scene importers.
    AmbientOcclusion and Lightmaps samples use also OpenGL 2.0 based LightsprintGL.
    Left: CPULightmaps. Right: AmbientOcclusion, Lightmaps.
  <table border=0 width=95%><tr align=top><td>
  \image html Integration1c.png
  </td><td>
  \image html Integration1b.png
  </td></tr></table>

 <table width=100% border=0 cellpadding=3 cellspacing=0><tr align=left style="background-color:#dddddd"><td>
 Steps:
 </td></tr></table>

  - <b> \subpage integration_step_1 </b> (important for all Lightsprint use cases, not only for this sample)
  - Convert scenes to Collada 1.4 (.DAE).
  - If you plan to compute maps rather than per-vertex values, include unwrap in second uv channel.
  - Change name of scene in sample .cpp, so it loads your scene.

 <table width=100% border=0 cellpadding=3 cellspacing=0><tr align=left style="background-color:#dddddd"><td>
 Expected time spent:
 </td></tr></table>

  - 1 day


\page integration_tool_2 Precalculations in your scene accessed in memory

  If you prefer other data source to Collada,
  use your scene loader and write adapter for accessing your scene in memory.

 <table width=100% border=0 cellpadding=3 cellspacing=0><tr align=left style="background-color:#dddddd"><td>
 Recommended approach:
 </td></tr></table>

  - Modify AmbientOcclusion (or CPULightmaps) to access scenes loaded to memory by your loader.

 <table width=100% border=0 cellpadding=3 cellspacing=0><tr align=left style="background-color:#dddddd"><td>
 Scheme of application:
 </td></tr></table>

  - Modified AmbientOcclusion newly depends on your scene loading code.
  \image html Integration2.png

 <table width=100% border=0 cellpadding=3 cellspacing=0><tr align=left style="background-color:#dddddd"><td>
 Steps:
 </td></tr></table>

  - Open AmbientOcclusion project
  - Copy RRObjectCollada.* to RRObjectCustom.* and add it to the project
  - Delete all FCollada \#includes from RRObjectCustom.h
  - Build reports errors on all references to FCollada,
    change code so it works with your engine's 3d model rather than with FCollada.
    Replace all references to FCollada with your custom format.
    For more details, see
    - comments in RRObjectCustom source code
    - <b> \subpage integration_step_2 </b>
    - <b> \subpage integration_step_3 </b>
    - interfaces of RRMesh and RRObject you implement
  - Decrease calculation quality for adapter testing,
    after short calculation
    you'll see your scene with rough ambient occlusion computed.
  - If you need GI lightmaps rather than AO, tweak sample to load your skybox,
    enable lights etc.


 <table width=100% border=0 cellpadding=3 cellspacing=0><tr align=left style="background-color:#dddddd"><td>
 Expected time spent:
 </td></tr></table>

  - 2-5 days




\page integration_realtime_1 Realtime GI in Lightsprint renderer

 You can immediately render realtime global illumination in your 3d scenes
 if you convert them to supported format and load into existing sample application.

 <table width=100% border=0 cellpadding=3 cellspacing=0><tr align=left style="background-color:#dddddd"><td>
 Recommended approach:
 </td></tr></table>

  - Convert your scene to Collada format
    and load it into Lightmaps (or load .3ds to RealtimeRadiosity) sample.

 <table width=100% border=0 cellpadding=3 cellspacing=0><tr align=left style="background-color:#dddddd"><td>
 Scheme of application:
 </td></tr></table>

  - Lightmaps and RealtimeRadiosity samples are
    built on top of purely numerical LightsprintCore, OpenGL 2.0 based LightsprintGL and scene importers.
    Left: Lightmaps. Right: RealtimeRadiosity.
  <table border=0 width=95%><tr align=top><td>
  \image html Integration1b.png
  </td><td>
  \image html Integration1a.png
  </td></tr></table>

 <table width=100% border=0 cellpadding=3 cellspacing=0><tr align=left style="background-color:#dddddd"><td>
 Steps:
 </td></tr></table>

  - <b> \subpage integration_step_1 </b> (important for all Lightsprint use cases, not only for this sample)
  - Convert scenes to Collada 1.4 (.DAE).
  - Change name of scene in Lightmaps.cpp, so it loads your scene.

 <table width=100% border=0 cellpadding=3 cellspacing=0><tr align=left style="background-color:#dddddd"><td>
 Expected time spent:
 </td></tr></table>

  - 1 day



\page integration_realtime_3 Realtime GI in your OpenGL renderer

  If you prefer other OpenGL renderer to LightsprintGL, switch to renderer of your choice.

 <table width=100% border=0 cellpadding=3 cellspacing=0><tr align=left style="background-color:#dddddd"><td>
 Recommended approach:
 </td></tr></table>

  - Modify RealtimeRadiosity sample to use your renderer,
    or modify your application/engine to use LightsprintCore and LightsprintGL.

 <table width=100% border=0 cellpadding=3 cellspacing=0><tr align=left style="background-color:#dddddd"><td>
 Scheme of application:
 </td></tr></table>

  - Application uses LightsprintCore and LightsprintGL to calculate GI,
    while your engine does scene loading and rendering.
  \image html Integration3.png

 <table width=100% border=0 cellpadding=3 cellspacing=0><tr align=left style="background-color:#dddddd"><td>
 Steps:
 </td></tr></table>

  - Make sure your renderer supports realtime shadows other than stencil based, e.g. shadowmapping
    or projected shadows. If it doesn't, add them or contact us for further support.
    Commercial licence to Lightsprint contains full source code of LightsprintGL,
    including realtime penumbra shadows.
  - <b> \subpage integration_step_4 </b>
  - <b> \subpage integration_step_5 </b>
  - Render our indirect illumination, see <b> \ref main_data_access </b>

 <table width=100% border=0 cellpadding=3 cellspacing=0><tr align=left style="background-color:#dddddd"><td>
 Expected time spent:
 </td></tr></table>

  - 1 week




\page integration_realtime_4 Realtime GI in any renderer

  If you want to use your renderer, but it is not OpenGL based,
  write custom GPU access routines as a replacement for LightsprintGL.

 <table width=100% border=0 cellpadding=3 cellspacing=0><tr align=left style="background-color:#dddddd"><td>
 Recommended starting point:
 </td></tr></table>

  - Modify your application/engine to use LightsprintCore.

 <table width=100% border=0 cellpadding=3 cellspacing=0><tr align=left style="background-color:#dddddd"><td>
 Scheme of application:
 </td></tr></table>

  - Different approaches: You can expose Lightsprint engine to your
    applications or hide it inside your engine.
  <table border=0 width=95%><tr align=top><td>
  \image html Integration4a.png
  </td><td>
  \image html Integration4b.png
  </td></tr></table>

 <table width=100% border=0 cellpadding=3 cellspacing=0><tr align=left style="background-color:#dddddd"><td>
 Steps:
 </td></tr></table>

  - <b> \subpage integration_step_6 </b>
  - for dynamic objects, implement RRIlluminationEnvironmentMap
  - for lightmaps, implement RRIlluminationPixelBuffer and reimplement
    RRDynamicSolver::newPixelBuffer() to return your class
  - for other than OpenGL API,
    you may need custom implementation of RRIlluminationVertexBuffer,
    implement it and reimplement
    RRDynamicSolver::newVertexBuffer() to return your class

 <table width=100% border=0 cellpadding=3 cellspacing=0><tr align=left style="background-color:#dddddd"><td>
 Expected time spent:
 </td></tr></table>

  - 1-3 weeks






\page integration_step_1 Validate static meshes

 This step is typically executed by 3d artists.

 Calculation produces better results
 when static 3d triangle meshes satisfy these conditions:

 \section cond_matching Cleanly connected triangles
 Two triangles are disjunct,
 share 1 vertex or share 1 edge and 2 vertices.

 So it is not allowed to
 - overlap triangles
 - intersect triangles
 - place edge in the middle of other triangle
 - place vertex in the middle of other edge

 \image html triangles.png

 See that all forbidden cases can be easily fixed.
 This operation can be automated.

 \section cond_no_needles No needles

 Angle sizes in triangles should be greater than 5 degrees.

 \section cond_no_hihpoly Prefer maps over high poly

 Polygon counts in dynamic objects have no influence on speed,
 but excessive polygon counts in static scene reduce performance.
 So for static scene,
 prefer normal maps or other tricks that reduce polygon count.






 
\page integration_step_2 Detect material properties

 Realtime global illumination is based on physically correct calculation of light transport.
 For such calculation it's necessary to know physical properties of surfaces.
 See RRMaterial for list of supported material properties of a surface.

 Fortunately you don't have to study advanced laws of physics for good results,
 you don't even need any additional information provided by artists who
 create materials or compose shaders. Everything can be detected automatically.

 In <b>special case</b> of RealtimeRadiosity example, materials contain diffuse texture
 without specular and transparency, so whole detection was reduced to calculating
 average color of diffuse texture and storing it into RRMaterial::diffuseReflectance.
 This took 1 hour.

 In <b>more general case</b> with known set of shaders,
 you can extend this approach and get all values by analyzing textures and other inputs
 used by shaders.
 This can take you few hours.
 Viability of this approach highly depends on your shaders and can't be decided here.
 If you are not sure, describe us your shaders and we will help you.

 In <b>fully general case</b>, you don't need any information about shaders.
 It works even if you let modders create new shaders, their properties will be autodetected.
 Everything you need is ability to render simple scene into small texture (16x16 pixels
 is typically enough)
 and calculate average color of rendered image. Follow these steps for each material

 -# Create empty scene and place 1x1m rectangle covered by material in front of camera
    in 0.5m distance (so it covers whole viewport, but nothing more).
    If it has uv coordinates for textures, use whole texture space from 0,0 to 1,1 in rectangle.
 -# Clear to black and render rectangle.
    Store acquired average color as RRMaterial::diffuseEmittance.
 -# Clear to white and render rectangle.
    Store acquired average color minus emittance as RRMaterial::specularTransmittance.
 -# Add white point light without distance attenuation to the same position as camera.
    Clear to black and render rectangle.
    Store acquired average color minus emittance as RRMaterial::diffuseReflectance.
 -# Call RRMaterial::validate() to clamp values to physically correct bounds.
    This is necessary to prevent damage from materials that e.g. reflect more light
    than they receive. Such behaviour is not possible in real world and may cause
    unexpected results in calculation such as infinitely high indirect illumination.

 -  If you use materials with faked reflection maps (planar or cubic),
    make sure that reflection intensity drops to 0 in dark scene,
    or manually disable it before detection.
    If you apply faked reflection even in completely dark unlit scene, detection described above
    must think it's emissive material.
    Alternatively, if you don't have any emissive materials, you can simply set emissivity to 0.

 This automatic approach can be further extended to differentiate between
 diffuse and specular reflectance or even to detect complete BRDF.

 Of course you are allowed to use any other approach, e.g. let graphics artists
 enter all values by hand.



\page integration_step_3 Create object wrappers

 Lightsprint transports light between object surfaces,
 so it needs to know everything
 about scene geometry and material properties.

 Lightsprint is designed to access your structures in arbitrary format,
 so you don't have to duplicate any data.
 This may save you huge amounts of memory, however,
 you must provide wrappers that access your structures.

 Further saving is possible thanks to geometry instancing.
 Multiple objects with different materials and different illumination
 may share one mesh and collider.

 \section import_without_instancing Import without instancing

 For import without geometry instancing,
 see samples/Import3DS or samples/ImportQuake3.
 It is everything necessary to load 3ds or Quake3 scene into 
 RRDynamicSolver.

 For sake of simplicity, wrappers duplicate some data in memory
 and don't support geometry instancing.
 On the other hand, support for custom data is demonstrated
 on diffuse textures and uv coordinates.

 Both RRMesh and RRObject interfaces are implemented in one class,
 so one RRMesh can't be shared by multiple RRObject-s
 and instances are not supported.

 \section import_with_instancing Import with instancing

 For import with instancing, see samples/ImportCollada.
 It is wrapper for scenes loaded by freely available FCollada library.

 Wrappers don't duplicate any memory, all data are accessed directly
 in FCollada document.

 RRMesh and RRObject interfaces are implemented in two separated classes,
 and one RRMesh is shared by multiple RRObject-s, if scene contains instances.

 \section import_new New importer

 To import data in your format, pick one of existing importers
 and modify it to access your data.

 Alternatively, to write new importer from scratch, follow these steps:

 -  Create RRMesh for every static triangle mesh in your engine.
    \n\n
    You can immediately create them 
    from trilists, tristrips, indexed trilists, indexed tristrips
    using single call to RRMesh::create() or RRMesh::createIndexed().
    For other formats ask for our support or implement your own RRMesh,
    which is very simple.
    \n\n
    Note that these wrappers don't duplicate your data
    and they support mesh optimizations and mesh aggregation,
    see RRMesh for details.
    \n\n
    Once you have RRMesh instance, create RRCollider using
    RRCollider::create().
    RRCollider::IT_LINEAR technique with minimal overhead is sufficient 
    for current version of RRDynamicSolver.
    \n\n
    Most convenient way to remember collider for later use is to
    attach it to your triangle mesh.
    You don't need to store pointer to RRMesh,
    it is available from collider using RRCollider::getMesh().
    \n\n
    RRDynamicSolver may use collider to calculate ray-mesh collisions,
    but it is available also to you via RRCollider::intersect().
    If you plan to use it and its performance is critical, use 
    other technique than RRCollider::IT_LINEAR.

 -  Implement your own RRObject and
    create its instance for every static object in your scene.
    \n\n
    By objects we understand identical meshes placed on different
    positions in scene, possibly using different materials (e.g. several
    cars with the same geometry, different position and possibly different color;
    they are different objects sharing one mesh).
    \n\n
    Default RRMesh::getTriangleMapping() returns realtime generated
    unwrap of low quality, for serious use of ambient maps, you should override
    RRMesh::getTriangleMapping() and provide your own unwrap.
    This is not necessary if you use vertex arrays and don't need lightmaps.
    \n\n
    Once you have RRObject instance, most convenient way to remember it
    for later use is to attach it to your object.





\page integration_step_4 Subclass RRDynamicSolver

 RRDynamicSolver implements generic realtime global illumination,
 without direct access to your renderer.
 Your task is to subclass RRDynamicSolver and implement
 tasks specific for your renderer.

 \section step4_gl OpenGL

 As a helper, there is RRDynamicSolverGL,
 subclass that already implements generic GPU tasks using OpenGL.
 If you plan to use OpenGL renderer (LightsprintGL or any other),
 subclass RRDynamicSolverGL and implement only 2 or 3
 missing member functions:
 - RRDynamicSolver::detectMaterials() - it is often empty,
   because application rarely modify materials
 - rr_gl::RRDynamicSolverGL::setupShader() - set shader so that 
   direct light+shadows+emissivity are rendered, but other
   material properties (diffuse texture etc) are ignored
 - rr_gl::RRDynamicSolverGL::detectDirectIllumination() -
   if not up to date, update resources needed for scene render, e.g. shadowmaps

 See example implementation in RealtimeRadiosity sample.

 \section step4_dx Direct3D

 If you need global illumination in Direct3D renderer,
 it is possible to subclass directly RRDynamicSolver,
 and avoid any dependency on OpenGL.
 The only additional task in this scenario (compared to OpenGL scenario above) is
 described in \ref integration_step_6





\page integration_step_5 Use RRDynamicSolver subclass

 RRDynamicSolver class is your primary interface to Lightsprint engine.
 To add global illumination to your application,

 - Create instance of your RRDynamicSolver subclass.
   (why subclass? described in \ref integration_step_4)

 - To start calculation,
   call RRDynamicSolver::setStaticObjects() with set of static objects participating in calculation.
   This call is expensive, design your application to avoid
   frequent changes of static scene.

 - Call RRDynamicSolver::calculate() often. If main loop of your
   application contains rendering of one frame, add one call to RRDynamicSolver::calculate().
   If you render scene only when it has changed,
   still call RRDynamicSolver::calculate() in every iteration of main loop,
   but if it returns IMPROVED, rerender scene.

 - If you use vertex/pixel buffers for rendering:
   \n When RRDynamicSolver::getSolutionVersion() changes,
   call RRDynamicSolver::updateVertexBuffers() or RRDynamicSolver::updateLightmaps()
   to update illumination values stored in vertex/pixel buffers.

 - Call RRDynamicSolver::reportDirectIlluminationChange() whenever direct illumination changes.
   It is mainly when light moves or changes properties, but it is also when
   object moves and its shadow changes.

 - Call RRDynamicSolver::reportMaterialChange() whenever materials used in scene change.

 - Call RRDynamicSolver::reportInteraction() whenever user interacts or other reason for
   high responsiveness exists. Without reportInteraction calls, solver takes more CPU time
   and FPS decreases.

 - Call RRDynamicSolver::getIllumination() 
   to acquire static object's illumination, see \ref data_vertex_buffer and \ref data_pixel_buffer.

 - Call RRDynamicSolver::updateEnvironmentMaps()
   to acquire static or dynamic object's illumination, see \ref data_environment_map.

 Nearly all of these calls are demonstrated in RealtimeRadiosity sample.





\page integration_step_6 Detect direct illumination

 LightsprintGL implements detection of direct illumination
 suitable for OpenGL based renderers -
 rr_gl::RRDynamicSolverGL::detectDirectIllumination().

 If you want to use Lightsprint with Direct3D renderer,
 you need to reimplement detection of direct illumination, write
 Your_subclass_of_RRDynamicSolver::detectDirectIllumination().

 <b>What needs to be detected:</b> average color of each face;
 how does it look lit by your direct (not ambient) point, spot and directional lights
 and shadowed by your shadows.

 <b>Why is it important:</b> we have no other knowledge about your lights.
 You can use many light types with very complex lighting equations.
 You can arbitrarily change them. No problem. Just let us know what are the results -
 average colors produced by your shader.

 <b>How to implement it:</b> turn off ambient/radiosity, render all scene faces,
 read rendered image to system memory and extract average color for each face.
 This is the most simple and universal approach (works with any number of any lights),
 however you can think about alternatives. This depends highly on your renderer.

 \image html detect-dif.png
 Image shows arrangement of faces in matrix that makes extraction of average color/exitance simple.
 It wouldn't help to render faces in their original 3d positions, some would be probably
 hidden behind other faces. You can see that some faces are black, those are partially
 in shadow or completely unlit.
 You can also see that only 50% of texture space is used, triangles may be rearranged so that
 100% of space is used, however averaging face color would become more expensive.

 <b>Possible implementation in detail:</b> rendering faces into matrix
 requires one renderer enhancement - 2d position override - ability to render
 triangles to specified 2d positions while preserving their original look.

 For DX10 generation GPUs, solution is nearly as simple
 as adding two lines into geometry shader.
 Render as usual, using any combination of trilist/strip/indexed/nonindexed data,
 but at the end of geometry shader, override
 output vertex positions passed to rasterizer by new 2d triangle positions
 calculated right there from primitive id.

 For DX9 generation GPUs, implemention has two steps:
 - Triangle positions in matrix are generated by CPU into new vertex stream.
   At the end of vertex shader, vertex position passed to fragment shader is replaced by
   position read from additional vertex stream.
 - For purpose of detection, scene is rendered using non-indexed triangle list.
   This is necessary because if we want to render triangles with shared
   vertices to completely different positions in texture, we have to split 
   that vertices.

 <i>
 <b>Possible optimizations:</b>

 <b>Rendering irradiance:</b>
 For simple materials with diffuse texture only (such as in .3ds),
 process can be optimized by ignoring diffuse textures, thus rendering incoming light
 not multiplied by diffuse texture.
 Detected average face colors then correspond to intensity of light reaching 
 face (irradiance) instead of intensity of light leaving face (exitance),
 so detected color is passed to RRObjectAdditionalIllumination::setTriangleAdditionalMeasure
 as RM_IRRADIANCE, rather than RM_EXITANCE.

 \image html detect-nodif.png
 Image above shows optimized detection, faces are not modulated by material,
 so irradiance is detected. You can see mostly white faces, because scene is lit by white spotlight.
 Few orange pixels come from orange logo projected by spotlight.

 Optimized approach is not suitable for engines with texture atlases.
 If your material properties change very significantly over uv space,
 use original unoptimized approach for higher precision.

 <b>GPU averaging:</b>
 To decrease amount of data transferred from GPU to CPU and speed up
 whole process, it is recommended to calculate average triangle colors
 on GPU, using simple shader, write them into smaller texture
 and transfer this smaller textue to CPU.
 LightsprintGL does it using scaledown_filter.* shaders.

 \image html detect-scaled.png
 Image was scaled down by scaledown_filter.* shaders for purpose of
 faster primary illumination detection.
 It is resized back to original size only in this documentation.

 </i>




\page main_data_access Calculation and outputs

 \section calc_1 Lightsprint can calculate
  - \subpage data_illumination
  - \subpage data_ambient_occlusion
  - \subpage data_bent_normals

 \section calc_2 and store them in object's
  - \subpage data_vertex_buffer
  - \subpage data_pixel_buffer
  - \subpage data_environment_map

 \section calc_0 Capabilities by speed
  - \subpage calc_realtime
    - \subpage calc_fireball
  - \subpage calc_offline

 Lightsprint is very flexible, it lets you pick optimal structures 
 for different objects and data types
 to precisely match your quality/memory footprint requirements.
 For example, you can use pixel buffers only for objects that benefit from per-pixel details,
 use vertex buffers for the rest. You can mix structures even in single object,
 e.g. use lightmap with per-vertex bent normals.

 Lightsprint automatically uses all processing power available in a single computer,
 it runs in multiple threads on all CPUs and CPU cores.

 Lightsprint is ready for distribution of work in network / multiple computers,
 including computers without GPU. Expensive RRDynamicSolver::updateLightmaps() call
 (it is core of all lightmap, ambient occlusion map and bent normal map calculations)
 can be replaced by many small RRDynamicSolver::updateLightmap() calls
 and you are free to execute them in parallel on different computers.



\page data_illumination Illumination

 \section di1 Directional lightmaps / vertex colors

  Optional directional component of lightmap or vertex color is stored separately 
  from irradiance component. See \ref data_bent_normals for
  details on directional component.

 \section di2 Global illumination lightmaps

  Global illumination lightmaps are precomputed with
  infinite light bounces, color bleeding and physically
  correct penumbra shadows from area lights.
  For mostly static scenes, precomputed GI lightmaps make very good sense.

  Global illumination usually contains sharp shadow edges,
  so it's not practical to store it per vertex in vertex buffer
  (result would be too blurry).

 \section di3 Precomputed ambient maps, per-vertex ambient

  Ambient maps and per-vertex ambient contain indirect component of illumination
  with infinite light bounces, color bleeding and indirect shadows;
  it's complete global illumination except for first light bounce.

  In scenarios with mixed static / dynamic objects, it's often
  advantageous to compute indirect illumination only
  and mix it with realtime rendered direct illumination
  with direct shadows.

  Per-pixel or per-vertex?
  \n
  Unlike global illumination, indirect illumination
  doesn't contain sharp shadow edges, so it's usually practical
  to store it per-vertex in vertex color buffer.
  Per-vertex ambient is slightly less precise in some situations,
  but it usually takes much less memory,
  so it's worth considering in memory restricted environments.
  In realtime scenarios, per-vertex ambient is preferred for
  higher speed over ambient maps.

  In non-realtime mode, all light source types are supported.
  In realtime mode, light source is implemented in custom 
  RRDynamicSolver::detectDirectIllumination() function.

  Ability to generate ambient maps or per vertex ambient is
  <b>fundamental for realtime global illumination</b>.
  To render realtime global illumination,
  start with renderer with direct illumination and shadows
  (e.g. LightsprintGL) and add ambient map or per vertex ambient 
  computed here. Realtime per-vertex ambient is preferred
  for much higher speed.

 \section di4 Environment maps

  Environment maps store light incoming from all directions
  to single point in space. They are usually used to approximate
  light incoming to dynamic object.
  \n
  Environment maps may be processed so that single environment
  map lookup returns light incoming from single direction
  (specular environment map)
  or from whole hemisphere (diffuse environment map).

  See \ref data_environment_map for more details.

 \section di5 Other

  Outside most widely used global illumination lightmaps,
  per vertex ambient, ambient maps and environment maps,
  Lightsprint supports many other illumination types suitable for specific
  situations.
  Depending on parameters you pass to lighting calculation process,
  you can get direct only lighting, mix of independently
  enabled direct and indirect lighting from many light source types
  etc, with result stored in texture or vertex buffer, with optional
  directional component.
            
 \section di6 Data structures

  Lightsprint can store object's illumination in

  - <b> \ref data_vertex_buffer </b>

  - <b> \ref data_pixel_buffer </b>

  - <b> \ref data_environment_map </b>

 \section di7 Calculation

  Calculation of illumination is realtime or non-realtime process,
  depending on parameters set.
  Environment maps are always computed in realtime speeds.

  See data structures (links above) for more details on calculation.

 \section di8 Use in renderer

   See data structures (links above) for more details on use in renderer.



\page data_ambient_occlusion Ambient occlusion

 \section da1 [Direct] ambient occlusion

  Ambient occlusion on <a href="http://en.wikipedia.org/wiki/Ambient_occlusion">Wikipedia</a>

  [Direct] ambient occlusion is a function of a surface point and a surrounding geometry,
  with return value in 0..1 space,
  0 for surface point fully occluded by surrounding geometry
  and 1 for completely unoccluded point.

  [Direct] ambient occlusion is a result of direct illumination
  in a uniform white environment.

 \section da2 Global ambient occlusion

  Sometimes ambient occlusion is desired to include effect of
  multiple light bounces and color bleeding.
  Let's call it global ambient occlusion.
  \n
  Global ambient occlusion is a function with return value in 0..inf space,
  but with typical values inside 0..1 range.
  \n
  Lightsprint supports global AO equally well as direct AO,
  so all that we say about AO matters for both direct and global AO.

 \section da3 Directional ambient occlusion

  Optional directional component of direct or global ambient occlusion is stored separately 
  from AO component. See \ref data_bent_normals for
  details on directional component.

 \section da3 Data structures

  Lightsprint can store object's ambient occlusion and global ambient occlusion in

  - <b> \ref data_vertex_buffer </b>

  - <b> \ref data_pixel_buffer </b>

 \section da4 Calculation

  Calculation of ambient occlusion is non-realtime process.
  To calculate ambient occlusion,
  - Create RRDynamicSolver
  - Set geometry with RRDynamicSolver::setStaticObjects().
    Set single object here for single object ambient occlusion.
    If you set scene with multiple objects here, each object will be
    occluded by all other objects.
  - Set uniform white environment with RRDynamicSolver::setEnvironment(RRIlluminationEnvironmentMap::createUniform())
  - Call RRDynamicSolver::updateLightmaps() for per-pixel
    or RRDynamicSolver::updateVertexBuffers() for per-vertex quality, with
    \n paramsDirect.applyCurrentSolution=false
    \n paramsDirect.applyLights=false
    \n paramsDirect.applyEnvironment=true
    \n and paramsIndirect=NULL
    \n To calculate global ambient occlusion,
       set paramsIndirect equally to paramsDirect.

  See data structures (links above) for more details on calculation.

 \section da5 Use in renderer

   See data structures (links above) for more details on use in renderer.



\page data_bent_normals Bent normals

 \section db1 Bent normals

  Bent normals on <a href="http://en.wikipedia.org/wiki/Ambient_occlusion">Wikipedia</a>

  Bent normals store negation of incoming light direction,
  in world space.
  Light may come to surface point form many directions
  in different intensities, so bent normals average all directions,
  taking light intensity into account.
  \n
  Bent normals are normalized.

 \section db2 Data structures

  Lightsprint can store object's bent normals in

  - <b> \ref data_vertex_buffer </b>

  - <b> \ref data_pixel_buffer </b>

  Bent normals stored in pixel buffer
  are automatically transformed from -1..1 space to 0..1 space to support
  save to unsigned RGB textures.
  When reading bent normal from such texture,
  get original world space normal as bn*2-1.

 \section db3 Calculation

  Calculation of bent normals is non-realtime process.
  To calculate bent normals, use RRDynamicSolver::updateVertexBuffers()
  or RRDynamicSolver::updateLightmaps()
  and set bent normal layer accordingly.

  Bent normals are intentionally stored in different layer than ilumination,
  so you can reuse single bent normal layer with multiple illumination layers
  to save memory or storage space (at cost of slightly reduced precision).

  See data structures (links above) for more details on calculation.

 \section db4 Use in renderer

  Bent normals are optional enhancement of precomputed illumination.
  They are used in rendering to make specular
  reflections and normal maps look more realistically,
  even with lighting precomputed in lightmaps.

  See data structures (links above) for more details on rendering.



\page data_vertex_buffer Vertex buffer

 Vertex buffer is designed for storage of per-vertex data for single object.

 \section d11 Suitable for
   <table>
    <tr><th></th>                                 <th>global or direct or indirect illumination</th> <th>ambient occlusion</th> <th>bent normals</th> </tr>
    <tr><th>static objects</th>                   <td>YES</td> <td>YES</td> <td>YES</td> </tr>
    <tr><th>dynamic objects</th>                  <td>NO </td> <td>YES</td> <td>YES</td> </tr>
    <tr><th>realtime calculated illumination</th> <td>YES</td> <td>NO </td> <td>NO </td> </tr>
    <tr><th>precalculated illumination</th>       <td>YES</td> <td>YES</td> <td>YES</td> </tr>
   </table>

 \section d12 Advantages
   - Compact representation. It requires only few bytes per vertex.
     Default implementation stores illumination as 3 floats, but you
     can easily add arbitrary compression.
   - Fast rendering.
     No sampler resources are consumed.
     It can be arbitrarily postprocessed in vertex shader.
     You can have multiple layers of precalculated indirect illumination 
     and mix them at no cost in vertex shader according to changes in scene.
   - There is no need to change your lightning equation,
     simply use our ambient data instead of constant ambient.
   - If you don't precompute bent normals, ambient lighting values
     don't depend on view angle, so rendering is very fast.
   - If you do precompute bent normals, normal maps work great even in shadows,
     knowing direction of incoming indirect light.

 \section d13 Disadvantages
   - Details are missing in areas without vertices.
     For good results, you have to add vertices to places where you miss details.
   - Seams around T vertices and other degenerated geometries.
     You have to make your meshes clean, avoid degeneracies.
   - Long narrow triangles (needles) often create visible artifacts.
     This is often problem also for physical engine,
     so your 3d artisis probably know they should avoid needles.

 \section d15 Interface, implementations
   - interface: RRIlluminationVertexBuffer
   - implementations: Platform independent vertex buffer is provided by 
     RRIlluminationVertexBuffer::createInSystemMemory().
     However, you are free to implement your own vertex buffer, see RRIlluminationVertexBuffer interface.
   - RRDynamicSolver::newVertexBuffer() uses
     RRIlluminationVertexBuffer::createInSystemMemory by default.

 \section d14 Instances
   - stored in: RRDynamicSolver::getIllumination()->getLayer()->vertexBuffer
   - created by: You may manually create an instance and write it to
     RRDynamicSolver::getIllumination()->getLayer()->vertexBuffer,
     but you don't have to,
     you can request RRDynamicSolver::updateVertexBuffers() to create missing buffers.
     It creates them by calling RRDynamicSolver::newVertexBuffer().
     Default RRDynamicSolver::newVertexBuffer() creates vertex buffer in system memory,
     plain array. It is platform independent and perfectly sufficient for OpenGL
     (glColorPointer accepts plain pointer to vertex color array in system memory).
     For other API, you may need different implementation, provide it by reimplementing
     RRDynamicSolver::newVertexBuffer() to return your class.
   - updated by: RRDynamicSolver::updateVertexBuffers()
     or RRDynamicSolver::updateVertexBuffer().

 \section d16 Rendering with
   - Stream data from vertex buffer into vertex shader, interpret them
     in shader appropriately as light level, ambient occlusion or bent normal.

 \section d17 Examples
   - OpenGL example:
     \n Rendering with per vertex ambient.
     \code
	rr::RRDynamicSolver* dynamicSolver;
	GLuint program;
	...
	// set program created from shaders below
	glUseProgram(program);
	// get vertex buffer with indirect illumination
	rr::RRIlluminationVertexBuffer* vertexBuffer = dynamicSolver->
		getIllumination(numberOfObject)->getLayer(0)->vertexBuffer;
	// enable stream with color values
	glEnableClientState(GL_COLOR_ARRAY);
	// set pointer to color data for first vertex
	glColorPointer(3, GL_FLOAT, 0, &vertexBuffer->lock()->x);
	// render primitives
	glDrawElements...
	// cleanup
	vertexBuffer->unlock();
	glDisableClientState(GL_COLOR_ARRAY);
     \endcode
     Using ambient value in GLSL vertex shader:
     \code
	varying vec3 ambientLight;
	void vertexShader()
	{
		...
		ambientLight = gl_Color;
	}
     \endcode
     Using ambient value in GLSL fragment shader:
     \code
	varying vec3 ambientLight;
	void fragmentShader()
	{
		...
		gl_FragColor = ... + materialColor * vec4(ambientLight.xyz,0.0);
	}
     \endcode
   - Direct3D 9 example:
     \n Rendering with per vertex ambient.
     \code
	IDirect3DDevice9* device;
	IDirect3DPixelShader9* vertexShader;
	IDirect3DPixelShader9* pixelShader;
	// adapt your vertex declaration, let your mesh read data from stream 0
	// and add e.g. COLOR1 read from stream 1
	IDirect3DVertexDeclaration9* vertexDeclaration;
	device->CreateVertexDeclaration(description, &vertexDeclaration);
	...
	// create d3d vertex buffer and fill it with vertexBuffer->lock() data
	rr::RRIlluminationVertexBuffer* vertexBuffer = dynamicSolver->
		getIllumination(numberOfObject)->getLayer(0)->vertexBuffer;
	IDirect3DVertexBuffer9* d3dBuffer = ...;
	// to prevent data duplication and copying, implement RRIlluminationVertexBuffer
	//  that stores data directly into d3d vertex buffer
	...
	// set rendering pipeline to use shaders below
	device->SetPixelShader(vertexShader);
	device->SetPixelShader(pixelShader);
	// activate previously created vertex declaration
	device->SetVertexDeclaration(vertexDeclaration);
	// set pointer to your mesh (vertices, possibly normals etc.) in stream 0
	device->SetStreamSource(0, ...);
	// set pointer to vertex illumination data in stream 1
	device->SetStreamSource(1, d3dBuffer, ...);
	// render primitives
	device->DrawPrimitive...
	// cleanup
	device->SetStreamSource(1, NULL, 0, 0);
	device->SetStreamSource(0, NULL, 0, 0);
     \endcode
     Using ambient value in HLSL vertex shader:
     \code
	void vertexShader(in float3 iAmbientLight: COLOR1,
		..., out float3 oAmbientLight: COLOR1)
	{
		...
		oAmbientLight = iAmbientLight;
	}
     \endcode
     Using ambient value in HLSL pixel shader:
     \code
	void pixelShader(in float3 iAmbientLight: COLOR1,
		..., out float4 oColor: COLOR)
	{
		...
		oColor = ... + materialColor * float4(iAmbientLight,0);
	}
     \endcode
   - Alternatively, applying colors from vertex buffer could be done in fixed pipeline,
     without shaders, but it is beyond scope of this documentation.
   - See Direct3D, OpenGL or your engine documentation for more details
     on streaming per vertex data to vertex shader and rendering with ambient light,
     ambient occlusion or bent normals.



\page data_pixel_buffer Pixel buffer

 Pixel buffer (2d texture) is designed for storage of per-pixel data 
 for single object (Light map, Ambient occlusion map, Bent normal map).

 \section d21 Suitable for
   <table>
    <tr><th></th>                                 <th>global or direct or indirect illumination</th> <th>ambient occlusion</th> <th>bent normals</th> </tr>
    <tr><th>static objects</th>                   <td>YES</td> <td>YES</td> <td>YES</td> </tr>
    <tr><th>dynamic objects</th>                  <td>NO </td> <td>YES</td> <td>YES</td> </tr>
    <tr><th>realtime calculated illumination</th> <td>NO </td> <td>NO </td> <td>NO </td> </tr>
    <tr><th>precalculated illumination</th>       <td>YES</td> <td>YES</td> <td>YES</td> </tr>
   </table>

 \section d22 Advantages
   - High precision and detail without additional vertices.
   - No need for good triangulation.
   - Very low resolution is sufficient (with good unwrap) for ambient maps
     (lightmaps with indirect illumination).
     Ambient maps contain mostly low frequencies, no sharp edges.
   - If you don't precompute bent normals, ambient lighting values
     don't depend on view angle, so rendering is very fast.
   - If you do precompute bent normals, normal maps work great even in shadows,
     knowing direction of incoming indirect light.

 \section d23 Disadvantages
   - You need additional uv channel with object's unwrap (for lightmap mapping).
     If you don't have it, ask your 3d artists
     to bake unwrap into meshes as an additional uv channel.
     Unwraps are often genrated automatically, using existing free or commercial tools.

 \section d25 Interface, implementations
   - interface: RRIlluminationPixelBuffer
   - implementations: OpenGL pixel buffer is available in
     rr_gl::RRDynamicSolverGL::createIlluminationPixelBuffer().
     For Direct3D, you are expected to implement
     your own pixel buffer, see RRIlluminationPixelBuffer interface.

 \section d24 Instances
   - stored in: RRDynamicSolver::getIllumination()->getLayer()->pixelBuffer
     or your arbitrary location.
   - created by:
     You may create pixel buffers manually at any time (and store them
     into RRDynamicSolver::getIllumination()->getLayer()->pixelBuffer
     or your arbitrary location), but you don't have to,
     you can request RRDynamicSolver::updateLightmaps() to create missing buffers.
     It creates them by calling RRDynamicSolver::newPixelBuffer().
     Default RRDynamicSolver::newPixelBuffer() returns NULL,
     so no pixel data are created and stored.
     You may reimplement RRDynamicSolver::newPixelBuffer() to
     return valid instances.
   - updated by: RRDynamicSolver::updateLightmaps()
     or RRDynamicSolver::updateLightmap().

 \section d26 Rendering
   - Map pixel buffer to your object using your uv channel with object's unwrap,
     read per-pixel value from texture in pixel shader
     and interpret it appropriately as light level, ambient occlusion or bent normal.

 \section d27 Examples
   - Generic example:
     \n Providing access to unwrap in your implementation of rr::RRMesh interface.
     \code
	// access to uv channel with object's unwrap
	void YourImplementationOfRRMesh::getTriangleMapping(
		unsigned t, TriangleMapping& out) const
	{
		for(unsigned v=0;v<2;v++)
		{
			// copy uv baked with your mesh
			// for vertex v (v=0..2) in triangle t
			out.uv[v][0] = ...; // u coordinate
			out.uv[v][1] = ...; // v coordinate
		}
	}
     \endcode
   - OpenGL example:
     \n Creating lightmap.
     \code
	virtual rr::RRIlluminationPixelBuffer*
	YourImplementationOfDynamicSolverGL::newPixelBuffer(rr::RRObject* object)
	{
		return createIlluminationPixelBuffer(256,256);
	}
     \endcode
     Rendering with lightmap.
     \code
	rr::RRDynamicSolver* dynamicSolver;
	GLuint program;
	...
	// set program created from shaders below
	glUseProgram(program);
	// bind lightmap to texture0
	glActiveTexture(GL_TEXTURE0);
	dynamicSolver->getIllumination(numberOfObject)->
		getLayer(0)->pixelBuffer->bindTexture();
	// set sampler to use texture0
	glUniform1i(glGetUniformLocation(program,"lightmap"),0);
	// enable stream with texture coordinates
	glEnableClientState(GL_TEXTURE_COORD_ARRAY);
	// set pointer to texture coordinates
	glColorPointer(2, GL_FLOAT, 0, array with uv values of unwrap);
	// render primitives
	glDrawElements...
	// cleanup
	glDisableClientState(GL_TEXTURE_COORD_ARRAY);
     \endcode
     Using uv coordinates in GLSL vertex shader:
     \code
	varying vec2 lightmapCoord;
	void vertexShader()
	{
		...
		lightmapCoord = gl_TexCoord.xy;
	}
     \endcode
     Sampling and using illumination value in GLSL fragment shader:
     \code
	uniform sampler2D lightmap;
	varying vec2 lightmapCoord;
	void fragmentShader()
	{
		vec4 light = texture2D(lightmap, lightmapCoord);
		...
		gl_FragColor = ... + materialColor * light;
	}
     \endcode
   - Direct3D 9 example:
     \n Creating lightmap.
     \code
	virtual rr::RRIlluminationPixelBuffer*
	YourImplementationOfRRDynamicSolver::newPixelBuffer(rr::RRObject* object)
	{
		// use your implementation of rr::RRIlluminationPixelBuffer
		return new YourImplementationOfRRIlluminationPixelBuffer(...);
	}
     \endcode
     Rendering with lightmap.
     \code
	IDirect3DDevice9* device;
	IDirect3DPixelShader9* vertexShader;
	IDirect3DPixelShader9* pixelShader;
	// create vertex declaration that includes lightmap uv channel as TEXCOORD0
	IDirect3DVertexDeclaration9* vertexDeclaration;
	device->CreateVertexDeclaration(description, &vertexDeclaration);
	rr::RRDynamicSolver* dynamicSolver;
	...
	// set rendering pipeline to use shaders below
	device->SetPixelShader(vertexShader);
	device->SetPixelShader(pixelShader);
	// set sampler to use lightmap
	//  (implement pixelBuffer class in d3d)
	dynamicSolver->getIllumination(numberOfObject)->
		getLayer(0)->pixelBuffer->bindTexture();
	// set vertex declaration for your mesh data,
	//  including uv channel with unwrap
	device->SetVertexDeclaration(vertexDeclaration);
	// set pointer to your mesh in stream 0
	//  (vertices, uv channel, possibly normals etc.)
	device->SetStreamSource(0, ...);
	// render primitives
	device->DrawPrimitive...
	// cleanup
	device->SetStreamSource(0, NULL, 0, 0);
     \endcode
     Using uv coordinates in HLSL vertex shader:
     \code
	void vertexShader(in float2 iLightmapCoord: TEXCOORD0,
		..., out float2 oLightmapCoord: TEXCOORD0)
	{
		...
		oLightmapCoord = iLightmapCoord;
	}
     \endcode
     Sampling and using illumination value in HLSL pixel shader:
     \code
	sampler lightmap;
	void pixelShader(in float2 iLightmapCoord: TEXCOORD0,
		..., out float4 oColor: COLOR)
	{
		float4 light = tex2D(lightmap, iLightmapCoord);
		...
		oColor = ... + materialColor * light;
	}
     \endcode
   - Alternatively, texturing could be done in fixed pipeline,
     without shaders, but it is beyond scope of this documentation.
   - See Direct3D, OpenGL or your engine documentation for more details
     on texturing and rendering with lightmap.
  


\page data_environment_map Environment map

 Environment map (cube texture) is designed for global illumination of single object.

 \section d31 Suitable for
   - static objects: YES
   - dynamic objects: YES
   - realtime calculated illumination: YES
   - precalculated illumination: YES

 \section d32 Advantages
   - offers global illumination with both specular and diffuse reflections
   - object doesn't have to be part of static scene, no RRObject wrapper is required
   - calculation is independent to object complexity, quick even for extremely complex objects

 \section d33 Disadvantages
   - precision decreases with size of object, suitable for characters and items, not for buildings
   - complexity of objects doesn't matter, but count of environment map updates does,
     so if you need large clouds/crowds of dynamic objects visible all at once,
     share one environment map for several close objects and update it only once
     to save time

 \section d35 Interface, implementations
   - interface: RRIlluminationEnvironmentMap
   - implementations: OpenGL environment map is available in 
     RRDynamicSolverGL::createIlluminationEnvironmentMap().
     For Direct3D, you are expected to implement
     your own environment map, see RRIlluminationEnvironmentMap interface.

 \section d34 Instances
   - stored in: your arbitrary location
   - created by: you
   - updated by: RRDynamicSolver::updateEnvironmentMaps()

 \section d36 Rendering
   - Many rendering techniques are based on faked precomputed environment maps.
     Here you get realtime computed environment maps, and you are free to use them
     for any purpose.
   - Request environment (cube) map to be generated in center of your object.
     Environment map may be later used by GPU to add global illumination 
     to diffuse and specular surfaces close to given point in space.
   - We propose you several techniques:
     \n
     For rough surface with mostly <b>diffuse</b> reflection,
     read value from diffuse environment map, using 'surface normal' as a coordinate.
     This single instruction gives you global illumination of pixel.
     Multiply it by material diffuse color to get final color.
     Size 4 of environment map is sufficient for close objects and 2 for distant ones.
     \n
     For smooth surface with <b>specular</b> reflection,
     read value from specular environment map, using
     'eye direction reflected by surface' as a coordinate.
     These few instructions give you global illumination of pixel.
     Don't modulate it by material color unless you want to render
     exotic materials, you already have final color.
     Size 16 of environment map simulates smooth surfaces, size 4 simulates rough
     surface.
     \n
     You can use <b>specular map</b> to select per pixel which one
     of two techniques to use or how to mix both together.
     \n
     You can use <b>normal map</b> to modulate surface normal.
     Both diffuse and specular surfaces respond well to normal maps.
     \n
     LightsprintGL implements all of these techniques.
   - Global illumination can be further improved if you use <b>ambient occlusion map</b>
     for your dynamic object. Multiply global illumination read from environment map
     by ambient occlusion read from ambient occlusion map to get more precise result.
     Ambient occlusion maps are computed for example by AmbientOcclusion sample.

 \section d37 Examples
   - OpenGL example:
     \n Rendering with environment maps.
     \code
	rr::RRDynamicSolver* dynamicSolver;
	rr::RRVec3 center; // object's center position in world
	rr::RRIlluminationEnvironmentMap* diffuseEnvironmentMap;
	rr::RRIlluminationEnvironmentMap* specularEnvironmentMap;
	GLuint program;
	...
	// update environment maps
	dynamicSolver->updateEnvironmentMaps(center,16,
		16,specularEnvironmentMap,4,diffuseEnvironmentMap);
	// set program created from shader below
	glUseProgram(program);
	// bind diffuse environment map to texture0
	glActiveTexture(GL_TEXTURE0);
	diffuseEnvironmentMap->bindTexture(); // calls glBindTexture(GL_TEXTURE_2D,map);
	// set sampler to use texture0
	glUniform1i(glGetUniformLocation(program,"diffuseEnvironmentMap"),0);
	// bind specular environment map to texture1
	glActiveTexture(GL_TEXTURE1);
	specularEnvironmentMap->bindTexture(); // calls glBindTexture(GL_TEXTURE_2D,map);
	// set sampler to use texture1
	glUniform1i(glGetUniformLocation(program,"specularEnvironmentMap"),1);
	// render primitives
	glDrawElements...
     \endcode
     Applying environment maps in GLSL fragment shader:
     \code
	uniform samplerCube specularEnvironmentMap;
	uniform samplerCube diffuseEnvironmentMap;
	void fragmentShader()
	{
		// normal in world space, you may apply normal map here
		vec3 worldNormal = ...;
		// view vector in world space = position of fragment - position of camera
		vec3 worldView = ...;
		// reflected view vector in world space
		vec3 worldViewReflected = reflect(worldView,worldNormal);
		...
		gl_FragColor = ...
			// diffuse reflection
			+ materialDiffuseReflectance *
			  textureCube(diffuseEnvironmentMap, worldNormal)
			// specular reflection
			+ materialSpecularReflectance *
			  textureCube(specularEnvironmentMap, worldViewReflected);
	}
     \endcode
   - Direct3D 9 example:
     \n Rendering with environment maps.
     \code
	IDirect3DDevice9* device;
	IDirect3DPixelShader9* pixelShader;
	rr::RRDynamicSolver* dynamicSolver;
	rr::RRVec3 center; // object's center position in world
	rr::RRIlluminationEnvironmentMap* diffuseEnvironmentMap;
	rr::RRIlluminationEnvironmentMap* specularEnvironmentMap;
	...
	// update environment maps
	dynamicSolver->updateEnvironmentMaps(center,16,
		16,specularEnvironmentMap,4,diffuseEnvironmentMap);
	// set rendering pipeline to use shader below
	device->SetPixelShader(pixelShader);
	// set samplers to use environment maps
	// (implement environment map class in d3d)
	diffuseEnvironmentMap->bindTexture();
	specularEnvironmentMap->bindTexture();
	// render primitives
	device->DrawPrimitive...
     \endcode
     Applying environment maps in HLSL pixel shader:
     \code
	samplerCUBE specularEnvironmentMap;
	samplerCUBE diffuseEnvironmentMap;
	void pixelShader(..., out float4 oColor: COLOR)
	{
		// normal in world space, you may apply normal map here
		float3 worldNormal = ...;
		// view vector in world space = position of fragment - position of camera
		float3 worldView = ...;
		// reflected view vector in world space
		float3 worldViewReflected = reflect(worldView,worldNormal);
		...
		oColor = ...
			// diffuse reflection
			+ materialDiffuseReflectance *
			  texCUBE(diffuseEnvironmentMap, worldNormal)
			// specular reflection
			+ materialSpecularReflectance *
			  texCUBE(specularEnvironmentMap, worldViewReflected);
	}
     \endcode
   - See Direct3D, OpenGL or your engine documentation for more details
     on texturemapping and applying environment maps.



\page calc_fireball Fireball

 \section fb_features Features
  Fireball is realtime global illumination solver, it produces realistic indirect lighting in dynamic scenes.
  It is recommended for use in games.

  If you don't start Fireball,
  \ref calc_realtime works without any precalculations (and \ref calc_offline work too).
  \n If you start Fireball, only realtime lighting works, but it is 
  - faster (1.2-5x higher fps in standard situations)
  - smaller (needs only 50% of memory for the same quality)
  - produces higher quality lighting
  - doesn't allocate/fragment memory

 \section fb_precalc Precalculations
  Fireball uses precalculation phase in which static scene is analyzed and one file is saved.
  This is usually done by developer at development time, final game only loads the file.

 \section fb_calculation Calculation
  Fireball uses the same API as the rest of Lightsprint SDK.
  Usually no changes in code are needed to start using Fireball,
  except for one additional call, see:
  - RRDynamicSolver::buildFireball()
  - RRDynamicSolver::loadFireball()

 \section fb_outputs Outputs
  Fireball calculates indirect illumination and stores it into 
  \ref data_vertex_buffer for static objects and
  \ref data_environment_map for dynamic objects.

 \section fb_sample Sample
  Fireball is demonstrated in RealtimeRadiosity sample.
  \n When Fireball file is not found, it is built automatically.
  In this sample, it takes approximately 3 seconds.



\page calc_realtime Realtime lighting

 \section rt_features Features
  Lightsprint uses the same API for both realtime and \ref calc_offline.
  Naturally only subset of functions is fast enough to be used in realtime.
  \n Features designed for realtime use include:
  - calculate global illumination in dynamic scenes
  - update \ref data_vertex_buffer with indirect lighting for static objects
  - update \ref data_environment_map with indirect lighting for dynamic objects

 \section rt_precalc Optional precalculations
  Realtime rendering with global illumination is available immediately after
  new scene is loaded, no preprocessing/precalculations are needed.

  However, better results are possible with \ref calc_fireball.
  It uses precalculation phase in which static scene is analyzed.
  It takes some time, but then rendering is faster and quality higher.

 \section rt_sample Sample
  Realtime lighting is demonstrated in RealtimeRadiosity sample.
  You can comment out block that enables \ref calc_fireball to compare both realtime solvers.


 
\page calc_offline Offline calculations

 \section of_features Features
  All Lightsprint SDK features documented in \ref main_data_access are available
  for offline calculations.

  Note that some offline featues are disabled if you load \ref calc_fireball for realtime lighting.

 \section of_sample Samples
  Offline calculations are demonstrated in CPULightmaps and AmbientOcclusion samples.
  CPULightmaps is purely console application,
  AmbientOcclusion displays results in simple interactive viewer.



\page main_platforms Supported platforms

 Operating systems
 - Windows XP
 - Windows XP x64
 - Windows Vista
 - Windows Vista x64
 - ask for more

 CPUs
 - x86 compatible with SSE instructions
 - ask for more

 GPUs
 - NVIDIA GeForce 5xxx, 6xxx, 7xxx, 8xxx
 - AMD (ATI) Radeon 9500-9800, Xxxx, X1xxx, HD2xxx, HD3xxx
 - mobile versions of GPUs above (GeForce Go, Mobility Radeon)
 - subset of workstation versions (Quadro, FireGL)
 - for offline rendering, GPU is not needed

 3D APIs
 - OpenGL 2 (examples included)
 - Direct3D 9/10 (create device with D3DCREATE_FPU_PRESERVE. no examples yet)
 - for offline rendering, 3D API is not needed

 Library configurations
 - Release DLL
 - Debug DLL

 Compilers
 - Visual C++ 2005 SP1
 - Visual C++ 2003 (single threaded, without Collada support)
 - ask for more




 
\page main_inputs Inputs

 \section inputs_structures Data structures
  - \subpage inputs_light_sources
  - \subpage inputs_geometry
  - \subpage inputs_materials

 \section inputs_files Files
  - \subpage inputs_collada
  - \subpage inputs_3ds
  - \subpage inputs_quake3
  - \subpage inputs_images

\page inputs_light_sources Light sources

 Lightsprint supports multiple types of light sources.

 All light sources support
 - HDR, floating point intensities (inputs, outputs)
 - direct, indirect or global illumination with infinite light bounces (outputs)
 - directional information (output)

 \section llights RRLights
  - Point/spot/directional light source.
  - Data structure: RRLight
  - Data structure container: RRLights
  - Setup: RRDynamicSolver::setLights()
  - Supported in offline GI: yes
  - Supported in realtime GI: yes

  - Point, spot and directional lights are implemented in RRLight.
    You can create new type, see RRLight interface.
    However, light must be emited by single point; to simulate area light,
    use e.g. triangles with emissive material.

 \section lenv Environment / Sky
  - Area light source, emissive skybox or sphere surrounding whole scene.
  - Data structure: RRIlluminationEnvironmentMap
  - Data structure container: none, only one environment in scene
  - Setup: RRDynamicSolver::setEnvironment()
  - Supported in offline GI: yes
  - Supported in realtime GI: no

 \section lemissive Emissive materials
  - Area light source, triangles covered by materials that emit light.
  - Data structure: RRObject with emissive materials returned by RRObject::getTriangleMaterial()
  - Data structure container: RRObjects
  - Setup: RRDynamicSolver::setStaticObjects()
  - Supported in offline GI: yes
  - Supported in realtime GI: no

 \section lrealtime Custom lights
  - Per-triangle irradiances generated by custom shader (see RRDynamicSolverGL::setupShader())
    or custom function (see RRDynamicSolver::detectDirectIllumination()).
  - Supported in offline GI: yes
  - Supported in realtime GI: yes



\page inputs_geometry Geometry
 Scene geometry is specified by RRObjects - set of RRObject objects.
 \n Each object has its own transformation matrix and pointer to RRMesh triangle mesh,
 so object is an instance of mesh.
 \n Mesh specifies vertices, triangles, normals, unwrap and custom data.

\page inputs_materials Materials
 Materials are specified by RRMaterial.

 Objects provide two levels of material information.
 \n 1. Per-triangle, RRObject::getTriangleMaterial() returns average material properties of one triangle.
 \n 2. Per-pixel, RRObject::getPointMaterial() returns material properties of single point on object's surface.

 Calculations with per-triangle materials are faster, so solver uses per-pixel material only when
 its per-triangle version has sideBits[].pointDetails flag set (set by you).

 Materials must be physically valid. Use RRMaterial::validate() to validate your materials.
 
\page inputs_collada Collada (.DAE)
 Collada is one of the most flexible and most widely supported 3d scene file formats.
 It is well specified open standard with great support.

 Source code of Collada adapter is in: samples/ImportCollada/*.*
 \n It adapts Collada scene with geometry, materials and light to Lightsprint format.
 \n Scene is loaded by open source FCollada library, precompiled in bin/lib directories.

 See comments at the beginning of samples/ImportCollada/RRObjectCollada.cpp
 for more details on features supported.

\page inputs_3ds .3DS
 3DS is very old and popular 3d scene file format.
 It is closed format without official specification.
 It supports only one uv coordinates, so it is suitable for realtime lighting and precomputed
 per-vertex, but not for precomputed lightmaps.

 Source code of 3ds adapter is in: samples/Import3DS/*.*
 \n It adapts 3DS scene with geometry and materials for Lightsprint format.
 \n Scene is loaded by open source 3DS loader in the same directory.

\page inputs_quake3 Quake 3 (.BSP)
 Quake 3 .BSP is popular 3d scene format used by now open source game Quake 3.
 It supports only one uv coordinates, so it is suitable for realtime lighting and precomputed
 per-vertex, but not for precomputed lightmaps.

 Source code of Quake 3 adapter is in: samples/ImportQuake3/*.*
 \n It adapts BSP scene with geometry and materials for Lightsprint format.
 \n Scene is loaded by open source BSP loader in the same directory.

\page inputs_images Images (jpg, png, dds, hdr, gif, tga...)
 Nearly all standard image formats are supported.
 \n Images with floating point colors are supported in .hdr format.

 Both 2d and cubemap images are supported.


 
 
\page main_ue3 Introduction for UE3 users

 This page should help you use Lightsprint SDK with Unreal Engine 3.
 \n Work is at the beginning, so you'll find only some details here,
 but more details and bigger picture should be added in next weeks/months.

 \section ue3_color Color

 Concept of color is implemented differently by both engines.
 First take a look at properties of any color type.
 - Color has channels (RGB, RGBA)
 - Color channel has precision (byte, float)
 - Color has scale (linear/physical, screen/sRGB)

 UE3 implements following types (with properties)
 - FColor (RGBA, byte, screen/sRGB)
 - FLinearColor (RGBA, float, linear/physical)

 Lightsprint implements following types (with properties)
 - RRColor, RRColorRGBF (RGB, float)
 - RRColorRGBA8 (RGBA, byte)
 - RRColorRGBAF (RGBA, float)

 Scale in Lightsprint is defined by context rather than by type -
 functions working with colors are scale neutral
 or they specify what scale they use.
 Two major scales exist in Lightsprint, linear/physical and custom.
 Custom scale is user-defined via RRDynamicSolver::setScale()
 and it is typically set to screen/sRGB RRScaler::createRgbScaler()
 which makes it compatible with FColor of UE3.

 So with different approach to scale in mind
 - FColor = RRColorRGBA8
 - FLinearColor = RRColorRGBAF

 
   
 
\page main_conventions Conventions

 \section gobjects Terminology
   <b>Mesh</b> is set of triangles with fixed positions in local space.
   \n\n
   <b>Object</b> is an instance of mesh, with position, rotation, scale
   and material properties.
   <b>Static object</b> never moves, rotates, deforms or changes material properties.
   <b>Dynamic object</b> freely changes these properties.
   \n\n
   <b>Scene</b> is set of objects.
   \n\n
   <b>Lightmap</b> is texture with irradiance values (incoming light)
   of object's surface, not modulated by material color.
   \n Lightmap could contain direct, indirect or global (both) illumination.
   \n Lightmap with indirect illumination is sometimes called <b>ambient map</b>.
   \n Irradiance values are typically stored in custom scale to fit 
   in 8bit precision, but physical or other HDR scale could be used too.

 \section glinking Automatic linking
   By default, including library header automatically links Release DLL version of library using \#pragma comment(lib,name).
   \n \#define RR_DEBUG / RR_GL_DEBUG selects Debug DLL instead of Release DLL.
   \n \#define RR_MANUAL_LINK / RR_GL_MANUAL_LINK disables automatic DLL linking.
   \n (version with RR_ affects LightsprintCore, RR_GL_ affects LightsprintGL)

 \section gunits Illumination units (radiometry, photometry, screen)
   Lightsprint computes all in HDR.
   Whole documentation talks in radiometry terms like irradiance,
   and Lightsprint internally works in radiometry units.
   All illumination measure inputs and outputs are 32bit float per component values.
   \n\n
   However, it is possible to communicate in screen colors
   or other units. Everything you need is to setup appropriate
   convertor, see RRDynamicSolver::setScaler(). Scaler internally converts values from native
   physical radiometry scale to your custom scale and vice versa.
   \n\n
   In typical situations, it is most straightforward to think and communicate
   in screen colors. This means you can set nearly all inputs in screen colors
   (scaled to 0-1 range) and read all outputs in screen colors.
   To setup this mode, call RRDynamicSolver::setScaler(RRScaler::createRgbScaler()).
   RealtimeRadiosity sample demonstrates it.

 \section gunits2 Geometry units
   All sizes and positions in space are expressed in generic units,
   Lightsprint doesn't need to know if it's meter, inch or parsec.
   However, some functions have default parameter values calibrated for
   human-sized scenes specified in meters (feature sizes roughly between
   0.01 and 100 units), so using meters may give you advantage in typical scenes.
   \n\n
   If it's possible, existing scene adapters adapt your scene from custom
   units to meters. (Source code of adapters is in samples/Import*)

 \section gscale Scale
   Lightsprint libraries support scaled objects.
   \n\n
   RRMesh and RRCollider support all scaled objects: positively or negatively, uniformly or non-uniformly scaled.
   \n\n
   RRObject and RRDynamicSolver support typical scaled objects:
   positively or negatively, uniformly scaled.
   \n Negative scale is supported with both possible interpretations
   for singlesided faces:
   Singlesided box visible from outside transformed with scale -1
   can stay visible form the outside or become visible only from inside,
   see RRObject::createWorldSpaceObject().

 \section gowner Ownership
   Dynamically created objects (using new) are never adopted, ownership never changes.
   \n This means that parameters that need to be destructed are never destructed inside call,
   responsibility for object is never passed to someone else.
   When you create object (using create() etc.), always delete it when it's
   no longer needed.

 \section gref Reference counting
   There is no internal reference counting, so if you create collider out of mesh,
   you are not allowed to destroy mesh before destroying collider. This danger should be
   mentioned on all appropriate places.

 \section gfinite Finite numbers
   If not otherwise specified, all inputs must be finite numbers.
   With Inf or NaN on input, result of any operation is undefined.

 \section gflodoub Floats and doubles
   Library uses both floats and doubles.
   It is not allowed to break double arithmetics by modifying FPU states.
   If you use Direct3D, make sure you create device with D3DCREATE_FPU_PRESERVE
   flag, otherwise it forces single precision and 
   breaks double precision arithmetics in whole program including dlls.

 \section gnull NULL
   Although NULL is obsoleted by C++ and some discourage from using it,
   we continue using it to distinguish zeros for pointers from zeros for non-pointers.
   So if you see x=0, x is NOT a pointer.
   If you see x=NULL, x IS a pointer.

 \section gmatrices Matrices
   Lightsprint uses 3x4 matrices for description of object transformation.
   See RRMatrix3x4 for explanation why we found it optimal.

 \section gup Up vector
   Although there is no limitation on orientation of 'up' vector,
   all samples work with 'up' in positive Y (0,1,0) and all scene adapters
   adapt scenes to 'up' in positive Y.
   Source code of adapters is in samples/Import*, so you can easily change up.






\page main_api API overview

 Lightsprint API consists of following libraries:

 - <b> \subpage api_core "LIGHTSPRINT CORE" </b> -
  Calculates global illumination in dynamic scenes.
  Includes fast ray-scene intersections.
  Is OpenGL/Direct3D independent.

 - <b> \subpage api_gl "LIGHTSPRINT GL" </b> -
  Integration of LightsprintCore with OpenGL 2.0.
  Renderer with penumbra shadows.

 See scheme of library and sample dependencies:

 \image html libraries.png

 See \ref main_samples for more details on samples.






\page api_core Lightsprint Core
 Lightsprint Core calculates global illumination in dynamic and static scenes.
 Subsystems include fast ray-scene intersections.
 It is OpenGL/Direct3D independent.

 Namespace: rr

 <hr>

 Header: RRDynamicSolver.h

 - calculates global illumination in static or dynamic scenes, offline or realtime
 - gives you full control over speed/quality
 - calculated illumination is available in vertex buffers, lightmaps and environment maps
 - communicates completely in custom units, e.g. screen colors
 - purely CPU, GPU based extensions implemented in rr_gl::RRDynamicSolverGL

 Samples RealtimeLights and RealtimeRadiosity show the result of integration,
 interactive Collada scene viewer with global illumination immediately responding
 to object and light movements.

 <hr>

 Headers: RRIllumination.h

 - storage for illumination calculated by RRDynamicSolver
 - illumination storage in lightmap: RRIlluminationPixelBuffer
 - illumination storage in vertex buffer: RRIlluminationVertexBuffer
 - illumination storage in environment map: RRIlluminationEnvironmentMap
 - storage of multiple illumination layers: RRObjectIllumination
 - allows for custom implementation -> smoothly integrates with other engines

 Sample Lightmaps shows illumination storage in action (saving/loading lightmaps etc).

 <hr>

 Header: RRObject.h

 - 3d object properties: geometry, materials, position etc

 <hr>

 Header: RRCollider.h

 - finds ray-mesh intersections
 - thread safe, you can calculate any number of intersections at the same time
 - you can select technique in range from maximal speed to zero memory allocated
 - up to 2^32 vertices and 2^30 triangles in mesh
 - builds helper-structures and stores them in cache on disk

 Sample HelloCollider shows the most simple usage scenario:
 -# Create RRMesh using your vertex/index buffers.
 -# Create RRCollider using your mesh.
 -# Create RRRay using your ray.
 -# Call RRCollider::intersect() to find intersections. Repeat for all rays.

 Sample BunnyBenchmark shows how to detect collisions on all available
 CPUs/cores at once.

 <hr>

 Header: RRMesh.h

 - interface to 3d triangle mesh
 - knows tristrips, trilists, indexed or not (RRMesh::create, RRMesh::createIndexed)
 - can optimize:
   - vertex welding (RRMesh::createOptimizedVertices)
   - removes degenerated triangles (RRMesh::createOptimizedTriangles)
 - merges many small meshes into one big mesh without additional memory (RRMesh::createMultiMesh)
 - saves/loads to disk (RRMesh::save, RRMesh::load)
 - extensible, you can add new channels like texture coords (RRChanneledData)
 - allows for procedural meshes, requires no memory (implementing your RRMesh takes few minutes)
 - up to 2^32-2 vertices and 2^32-2 triangles in mesh
 - thread safe, you can use mesh in any number of threads at the same time

 Sample HelloMesh shows the most simple usage scenario,
 mesh is created out of existing array of vertices.

 <hr>

 Header: RRMath.h

 - math classes used by whole Lightsprint SDK
 - RRReal holds one real number, which is single precision float
 - RRVec2 is vector of 2 real numbers
 - RRVec3 is vector of 3 real numbers
 - RRVec4 is vector of 4 real numbers

 <hr>

 Header: RRDebug.h

 - debugging and reporting routines used by whole Lightsprint SDK
 - RRReporter processess all messages sent by Lightsprint SDK to you




\page api_gl LightsprintGL

 LightsprintGL implements Lightsprint's access to GPU using OpenGL 2.0 API.
 Advanced renderer with penumbra shadows is included.
 Full source code comes with commercial licence.

 Namespace: rr_gl

 <hr>
 Header: GL/RRDynamicSolverGL.h

 - rr_gl::RRDynamicSolverGL implements RRDynamicSolver GPU specific parts in OpenGL 2.0
 - pixel buffers
 - environment maps

 <hr>
 Header: GL/RendererOfScene.h

 - rr_gl::RendererOfScene is a renderer of 3d scene in RRDynamicSolver
 - renders environment, HDR
 - renders scene objects with precomputed or realtime computed illumination
 - detects what data are available (lightmaps, vertex colors)

 <hr>
 Header: GL/RendererOfRRObject.h

 - rr_gl::RendererOfRRObject is a renderer of 3d objects with Lightsprint's RRObject interface
 - renders object with precomputed or realtime computed illumination

 <hr>
 Other headers (independent to LightsprintCore):

 - rr_gl::Texture is OpenGL texture
 - rr_gl::Program is GLSL program
 - rr_gl::UberProgram is GLSL program with preprocessor parameters changeable at runtime
 - rr_gl::UberProgramSetup is set of parameters for our ubershaders UberShader.vs and UberShader.fs
 - rr_gl::Camera is frustum suitable for camera or spotlight
 - rr_gl::RealtimeLight is an extension of RRLight, structures needed for realtime GI rendering
 - rr_gl::Renderer is generic renderer interface

 Features and limitations of LightsprintGL's renderer
   - for Windows (request support for other platforms)
   - requires OpenGL 2.0 capable GPU (GeForce 5xxx and higher, Radeon 9500 and higher)
   - spot and point lights with realtime shadows
   - linear and area spotlights with realtime penumbra shadows
   - polynomial, exponential and physically correct distance attenuation models
   - loading 2d and cube textures from jpg, png, dds, tga, tiff, gif and many more
   - saving 2d and cube textures to jpg, png, tga, tiff, gif and many more
   - separately enabled/disabled light features:
     - color
     - projected texture
     - several models of distance attenuation
     - shadows with variable softness, resolution
     - diffuse environment map with variable resolution
     - specular environment map with variable resolution
   - separately enabled/disabled material features:
     - diffuse color per vertex
     - diffuse map
     - specular (enabled for whole object)
     - specular map (specular modulated by diffuse map)
     - normal map (normal modulated by diffuse map)
     - emissive map





\page main_samples Samples

 \section samples_root Purpose

 - \subpage samples_gl
 - \subpage samples_core
 - \subpage samples_import



\page samples_core LightsprintCore - precalculations, collisions

 Following projects use LightsprintCore library for lighting or collision calculations.
 \n They are located in samples directory in the SDK, binaries in bin.
 \n Results are usually displayed in text console or saved to data/export directory.

 <hr>
 <b>CPULightmaps</b>
  <table border=0 width=95%><tr align=top><td>
  \image html samples/CPULightmaps_1.jpg computed lightmap
  </td><td>
  \image html samples/CPULightmaps_2.jpg computed bent normal map
  </td></tr></table>
 - precalculates lightmaps/vertexcolors from all types of lights (point/spot/dir/skybox)
 - no rendering, only CPU is used, runs on GPU-less machines
 - loads Collada scene including all lights

 <hr>
 <b>AmbientOcclusion</b>
  <table border=0 width=95%><tr align=top><td>
  \image html samples/AmbientOcclusion_1.jpg computed global ambient occlusion map
  </td><td>
  \image html samples/AmbientOcclusion_2.jpg with materials applied
  </td></tr></table>
 - precalculates ambient occlusion maps/vertexcolors with infinite light bounces and color bleeding
 - optional viewer of results (screenshots) is not important for calculation
 - loads Collada scene

 <hr>
 <b>BunnyBenchmark</b>
 - measures Collider performance for comparison with other engines
 - uses OpenMP to employ all available CPUs/cores
 - Collider results are up to 200x better than commercial physical engines

 <hr>
 <b>MultiMeshCollider</b>
 - ray-multimesh collision test, for static scenes with multiple meshes

 <hr>
 <b>HelloCollider</b>
 - the most simple case of ray-mesh collision test

 <hr>
 <b>HelloMesh</b>
 - the most simple case of mesh creation



\page samples_gl LightsprintGL - realtime rendering

 Following projects use LightsprintGL library for rendering
 and LightsprintCore for lighting calculations.
 \n They are located in samples directory in the SDK, binaries in bin.

 <hr>
 <b>RealtimeLights</b>
  <table border=0 width=95%><tr align=top><td>
  \image html samples/RealtimeLights_1.jpg freely move lights and objects
  </td><td>
  \image html samples/RealtimeLights_2.jpg occluded light, GI changes in realtime
  </td></tr></table>
 - realtime GI, color bleeding
 - all lights and objects movable
 - no precalculations -> for scene viewers, designers
 - loads collada scene including all lights, loads 3ds dynamic objects
 - uses internal renderer

 <hr>
 <b>RealtimeRadiosity</b>
  <table border=0 width=95%><tr align=top><td>
  \image html samples/RealtimeRadiosity_2.jpg freely move light, objects
  </td><td>
  \image html samples/RealtimeRadiosity_1.jpg global illumination changes in realtime
  </td></tr></table>
 - realtime GI, color bleeding, penumbra shadows
 - all lights and objects movable
 - optional precalculations -> for games
 - loads 3ds scene and 3ds dynamic objects, uses 1 custom area light rather than lights from file
 - shows that lighting works equally well for animated object
 - shows feeding external 3ds renderer with Lightsprint computed illumination

 <hr>
 <b>Lightmaps</b>
  <table border=0 width=95%><tr align=top><td>
  \image html samples/Lightmaps_1.jpg realtime GI, move light
  </td><td>
  \image html samples/Lightmaps_2.jpg precomputed quality (p pressed)
  </td></tr></table>
 - like RealtimeRadiosity, but computes lightmaps on request, see better shadows on right image
 - for analyzing errors of realtime lighting

 <hr>
 <b>PenumbraShadows</b>
  <table border=0 width=95%><tr align=top><td>
  \image html samples/PenumbraShadows_1.jpg penumbra shadows
  </td><td>
  \image html samples/PenumbraShadows_2.jpg penumbra shadows
  </td></tr></table>
 - renders realtime direct illumination in scene with dynamic objects and area light
 - penumbra shadows
 - simple, no global illumination, only constant ambient



\page samples_import Scene import

 Several sample 3d scene importers are provided.
 They are not libraries, only *.h and *.cpp files to be included in other projects.
 \n They are located in samples directory in the SDK.

 <hr>
 <b>ImportCollada</b>
 - source code of Collada adapter
 - \ref inputs_collada details

 <hr>
 <b>Import3DS</b>
 - source code of 3DS loader, renderer and adapter
 - \ref inputs_3ds details

 <hr>
 <b>ImportQuake3</b>
 - source code of Quake3 map loader and adapter
 - \ref inputs_quake3 details


*/

};
