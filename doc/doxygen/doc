namespace rr
{

/**


\mainpage Lightsprint SDK
 Welcome to Lightsprint SDK.

 - <b> \subpage main_introduction </b>

 - <b> \subpage main_scenarios </b>

 - <b> \subpage main_integration </b>

 - <b> \subpage main_data_access </b>

 - <b> \subpage main_conventions "Conventions (units, scale, floats...)" </b>

 - <b> \subpage main_platforms </b>

 - <b> \subpage main_api "API reference" </b>

 \image html all-rr.png




\page main_introduction Introduction

 \section intro1 Problem and Solution

  Realistic illumination simply looks good.
  While direct illumination has been 'solved' by science and software developers of 20th century,
  indirect illumination still challenges both sides. \subpage details

  Lightsprint is first to come with physically correct 
  indirect illumination synthesis so fast, that it 
  is suitable for realtime rendering in dynamic scenes.

 \section intro2 Your contribution

  Renderer with realtime direct illumination and shadows.

  There are many good renderers, commercial and free, proprietary and open source.
  If you don't have renderer, you can start with our renderer from DemoEngine.

  \image html all-ambient.png
  Today's engines ignore indirect illumination from moving lights
  and render only direct illumination (left column),
  or approximate indirect by constant ambient term (right column).

 \section intro3 Our contribution

  Lightsprint calculates physically correct indirect illumination
  in realtime as a replacement for constant ambient.

  \image html all-rr.png

  We can also precalculate complete direct+indirect illumination from area lights.




\page details More details
 In our real world, we see visible light coming mostly from special surfaces 
 (hot wolfram fibre in bulb, luminofor of fluorescent lamp)
 or from whole volumes of plasma (sun, fire).
 We call these surfaces and volumes <b>source of direct illumination</b>.

 Light from sources of direct illumination reaches other surfaces 
 where part of light gets absorbed and part reflected.
 Reflected part is what we see and what makes objects look lit by direct illumination.
 When we see the reflected part, we say that object has <b>direct illumination</b>.

 Reflected light from direct illumination reaches other surfaces, partially reflects,
 reaches other surfaces, partially reflects etc.
 When we see sum of these reflected parts, we say that object has <b>indirect illumination</b>.

 Illumination we see in real world is sum of direct and indirect illumination.

 Computer graphics tries to simulate this process in order to generate realistic images.
 This is however very time consuming process.
 So realtime computer graphics in 99% of cases resigns to real-world sources of direct 
 illumination and uses imaginary <b>"point", "spot" or "directional" lights</b>.
 These fictitious sources of direct illumination
 allow realtime computer graphics to calculate direct illumination very quickly.
 This process includes calculation of <b>shadows in direct illumination</b>.
 There are many realtime techniques for calculating shadows in direct illumination,
 most notably shadow mapping, volumetric/stencil shadows and projected/texture based shadows.

 There is still problem with indirect illumination, which remains
 hard to be computed quickly.




\page main_scenarios Usage scenarios

 \section scenario2 Realtime radiosity integrated into editor. Game uses precalculated illumination

  - Saves months of work to graphics artists and designers,
    previously spent in lightmap rebuilds or fake light placement.

  - Allows artists to find visually more attractive positions / settings for lights.

  - Saves months spent in adapting your data for external global illumination
    tools. Other tools don't support arbitrary materials and lights.

  - Simple path for incorporating realtime radiosity into future games.

 \section scenario1 Realtime radiosity integrated into game

  - Greatly improves realism and visual appeal.

  - Requires no level preprocessing, saves time previously spent on infinite levels builds.

  - Simplifies integration of mods and other user provided assets.

  - Supports arbitrary materials / shaders, even those writen by modders
    after game release.

  - Publicity, be first with this level of realism never seen before.

  - See <a href="http://dee.cz/rrb/RRBugs.rar">Realtime Radiosity Bugs</a>
    as an example. It's not a real game, but it shows innovative use 
    of realtime radiosity in game. It doesn't have to be just eye candy.
    
 \image html rrbugs_hint.jpg





\page main_integration Integration

 You can quickly start playing with your data
 if you convert them to .3ds and load into HelloRealtimeRadiosity.
 (Use 1m units, truecolor .tga textures and no transformations.)

 To integrate realtime radiosity into your editor, game or any other application,
 follow these steps:

 - <b> \subpage integration_step_0 </b>

 - <b> \subpage integration_step_1 </b>

 - <b> \subpage integration_step_2 </b>

 - <b> \subpage integration_step_5 </b>

 - <b> \subpage integration_step_6 </b>

 - Stop rendering constant ambient

 - Start rendering our illumination, see <b> \ref main_data_access </b>

 Concept behind realtime radiosity integration is also described in
 <a href="http://dee.cz/rri">Realtime Radiosity Integration article</a>.

 Final result of integration is demonstrated in HelloRealtimeRadiosity example.






\page integration_step_0 Validate static meshes

 This step is typically executed by 3d artists.

 Calculation produces better results
 when static 3d triangle meshes satisfy these conditions:

 \section cond_matching Cleanly connected triangles
 Two triangles are disjunct,
 share 1 vertex or share 1 edge and 2 vertices.

 So it is not allowed to
 - overlap triangles
 - intersect triangles
 - place edge in the middle of other triangle
 - place vertex in the middle of other edge

 \image html triangles.png

 See that all forbidden cases can be easily fixed.
 This operation can be automated.

 \section cond_no_needles No needles

 Angle sizes in triangles are above 5 degrees.

 \section cond_no_hihpoly Prefer maps over high poly

 Polygon counts in dynamic objects have no influence on speed,
 but excessive polygon counts in static scene reduce performance,
 prefer normal maps or other tricks that reduce polygon count.






 
\page integration_step_1 Detect material properties

 Realtime radiosity is based on physically correct calculation of light transport.
 For such calculation it's necessary to know physical properties of surfaces.
 See RRSurface for list of all properties.

 Fortunately you don't have to study advanced laws of physics for good results,
 you don't even need any additional information provided by artists who
 create materials or compose shaders. Everything can be detected automatically.

 In <b>special case</b> of HelloRealtimeRadiosity example, materials contain diffuse texture
 without specular and transparency, so whole detection was reduced to calculating
 average color of diffuse texture and storing it into RRSurface::diffuseReflectance.
 This took 1 hour.

 In <b>more general case</b> with known set of shaders,
 you can extend this approach and get all values by analyzing textures and other inputs
 used by shaders.
 This can take you few hours.
 Viability of this approach highly depends on your shaders and can't be decided here.
 If you are not sure, describe us your shaders and we will help you.

 In <b>fully general case</b>, you don't need any information about shaders.
 It will work even if you let modders create new shaders, their properties will be detected.
 Everything you need is ability to render simple scene into small texture (16x16 pixels
 is typically enough)
 and calculate average color of rendered image. Follow these steps for each material

 -# Create empty scene and place rectangle covered by material in front of camera
    (so it covers whole viewport, but nothing more).
    If it has uv coordinates for textures, use whole texture space from 0,0 to 1,1 in rectangle.
 -# Clear to black and render rectangle.
    Store acquired average color as RRSurface::diffuseEmittance.
 -# Clear to white and render rectangle.
    Store acquired average color minus emittance as RRSurface::specularTransmittance.
 -# Add white light without distance attenuation to the same position as camera.
    Clear to black and render rectangle.
    Store acquired average color minus emittance as RRSurface::diffuseReflectance.
 -# Call RRSurface::validate() to clamp values to physically correct bounds.
    This is necessary to prevent damage from materials that eg. reflect more light
    than they receive. Such behaviour is not possible in real world and may cause
    unexpected results in calculation such as infinitely high indirect illumination.

 This automatic approach can be further extended to differentiate between
 diffuse and specular reflectance or even to detect complete BRDF.

 Of course you are allowed to use any other approach, eg. let graphics artists
 enter all values by hand.





\page integration_step_2 Create object wrappers

 Realtime radiosity transports light between object surfaces, so it needs to know everything
 about scene geometry and surface properties.

 Realtime radiosity is designed to access your structures in arbitrary format,
 so you don't have to duplicate any data. This may save you huge amounts of memory,
 however you must provide wrappers that access your structures.

 For <b>simple case</b>, see src/DemoEngine/3ds2rr.cpp. It is source code
 of everything necessary to load .3ds model into realtime radiosity.
 It is simple because .3ds doesn't support geometry instancing.
 On the other hand, support for custom data is demonstrated
 on diffuse textures and uv coordinates.
 For sake of simplicity, it duplicates some data.

 For <b>fully general</b> engine with geometry instancing, follow these steps:

 -  Create RRMesh for every static triangle mesh in your engine.
    \n\n
    You can immediately create them 
    from trilists, tristrips, indexed trilists, indexed tristrips
    using single call to RRMesh::create() or RRMesh::createIndexed().
    For other formats ask for our support or implement your own RRMesh,
    which is very simple.
    \n\n
    Note that these wrappers don't duplicate your data
    and they support mesh optimizations and mesh aggregation,
    see RRMesh for details.
    \n\n
    Once you have RRMesh instance, create RRCollider using
    RRCollider::create().
    RRCollider::IT_LINEAR technique with minimal overhead is sufficient 
    for current version of RealtimeRadiosity.
    \n\n
    Most convenient way to remember collider for later use is to
    attach it to your triangle mesh.
    You don't need to store pointer to RRMesh,
    it is available from collider using RRCollider::getMesh().
    \n\n
    Realtime radiosity may use collider to calculate ray-mesh collisions,
    but it is available also to you via RRCollider::intersect().
    If you plan to use it and its performance is critical, use 
    other technique than RRCollider::IT_LINEAR.

 -  Implement your own RRObject and
    create its instance for every static object in your scene.
    \n\n
    By objects we understand identical meshes placed on different
    positions in scene, possibly using different materials (eg. several
    cars with the same geometry, different position and possibly different color;
    they are different objects sharing one mesh).
    \n\n
    Default RRObject::getTriangleMapping() returns realtime generated
    unwrap of low quality, for any serious use of ambient maps, you should override
    RRObject::getTriangleMapping() and provide your own unwrap.
    This is not necessary if you use vertex arrays instead of ambient maps.
    \n\n
    Once you have RRObject instance, most convenient way to remember it
    for later use is to attach it to your object.





\page integration_step_5 Detect direct illumination in RRRealtimeRadiosity subclass

 RRRealtimeRadiosity contains several abstract methods, derive your class from RRRealtimeRadiosity
 and implement these methods.

 Check example implementation in HelloRealtimeRadiosity.
 You can see that the only nontrivial task was implementing 
 RRRealtimeRadiosity::detectDirectIllumination().
 
 <b>What needs to be detected:</b> average color of each face;
 how does it look lit by your direct (not ambient) point, spot and directional lights
 and shadowed by your shadows.
 This is typical approach, but interface is much more flexible,
 so you can detect any of face irradiance, incoming flux,
 exitance or exiting flux; and you can use any physical or non-physical scale.

 <b>Why is it important:</b> we have no other knowledge about your lights.
 You can use many light types with very complex lighting equations.
 You can arbitrarily change them. No problem. Just let us know what are the results -
 average colors produced by your shader.

 <b>How to implement it:</b> turn off ambient/radiosity, render all scene faces,
 read rendered image to system memory and extract average color for each face.
 This is the most simple and universal approach (works with any number of any lights),
 however you can think about alternatives. This depends highly on your renderer.

 \image html detect-dif.png
 Image shows arrangement of faces in matrix that makes extraction of average color/exitance simple.
 It wouldn't help to render faces in their original 3d positions, some would be probably
 hidden behind other faces. You can see that some faces are black, those are partially
 in shadow or completely unlit.
 You can also see that only 50% of texture space is used, triangles may be rearranged so that
 100% of space is used, however averaging face color would become more expensive.

 <b>Possible implementation in detail:</b> rendering faces into matrix
 requires one renderer enhancement - 2d position override - ability to render
 triangles to specified 2d positions while preserving their original look.

 For DX10 generation GPUs (GeForce 8xxx), solution is nearly as simple
 as adding two lines into geometry shader.
 Render as usual, using any combination of trilist/strip/indexed/nonindexed data,
 but at the end of geometry shader, override
 output vertex positions passed to rasterizer by new 2d triangle positions
 calculated right there from primitive id.
 \n You can use GeForce 8800 and multivendor OpenGL extension
 EXT_geometry_shader4 to write geometry shaders.
 New graphics cards and the same functionality in Direct3D 10 are expected soon.

 For DX9 generation GPUs, implemention has two steps:
 - Triangle positions in matrix are generated by CPU into new vertex stream.
   At the end of vertex shader, vertex position passed to fragment shader is replaced by
   position readen from additional vertex stream.
 - For purpose of detection, scene is rendered using non-indexed triangle list.
   This is necessary because if we want to render triangles with shared
   vertices to completely different positions in texture, we have to split 
   that vertices.
   In case of HelloRealtimeRadiosity, this is handled by branch in renderScene():
   after setting shader, two paths for specifying vertex data follow,
   - m3ds.Draw is original indexed tristrip path
   - rendererCaching->render() is new non-indexed trilist path 
     written only for purpose of this detection

 <i>
 <b>Optional reading:</b>

 <b>Faster detection:</b>
 for simple materials with diffuse texture only (such as in .3ds)
 process can be optimized by ignoring diffuse textures, thus rendering incoming light
 not multiplied by diffuse texture.
 Detected average face colors then correspond to intensity of light reaching 
 face (irradiance) instead of intensity of light leaving face (exitance),
 so detected color is passed to RRObjectAdditionalIllumination::setTriangleAdditionalMeasure with RM_IRRADIANCE
 instead of RM_EXITANCE.

 \image html detect-nodif.png
 Image above shows optimized detection, faces are not modulated by material
 so irradiance is detected. You can see mostly white faces, because scene is lit by white spotlight.
 Few orange pixels come from orange logo projected by spotlight.

 Optimized approach is not suitable for engines with texture atlases.
 If your material properties change very significantly over uv space,
 use original unoptimized approach for higher precision.
 </i>





\page integration_step_6 Use RRRealtimeRadiosity subclass

 To finish integration with your application (editor or game),
 create instance of your RRRealtimeRadiosity subclass
 and call its several methods on appropriate places of your application
 as described here:

 To start radiosity calculation, call RRRealtimeRadiosity::setObjects()
 with set of objects participating in calculation.

 Call RRRealtimeRadiosity::calculate() often. If main loop of your
 application contains rendering of one frame, add one call to RRRealtimeRadiosity::calculate().
 If you render scene only when it has changed,
 still call RRRealtimeRadiosity::calculate() in every iteration of main loop,
 but if it returns true, take it as signal that scene has changed.

 Use "reportSomething" methods to report events, see RRRealtimeRadiosity for details.

 Use RRRealtimeRadiosity::getIllumination() to acquire object's indirect illumination during render.

 See example in samples/HelloRealtimeRadiosity/HelloRealtimeRadiosity.cpp.






\page main_data_access Data access
 Lightsprint calculates indirect illumination.
 To render using calculated illumination, you need to access calculated data structures.
 Lightsprint supports multiple data structures optimized for different use cases.

 Pick exactly one of these structures for each object
 and use it for rendering global illumination. You can use different structures
 for different objects.

 - <b> \subpage data_vertex_buffer </b>

 - <b> \subpage data_ambient_map </b>

 - <b> \subpage data_environment_map </b>

 For quick access to single value (for example by AI), you can query illumination for

 - <b> \subpage data_triangle </b>

 - <b> \subpage data_ray </b>



\page data_vertex_buffer Vertex color buffer

 \section d11 Suitable for
   - static objects: YES
   - dynamic objects: NO
   - realtime calculated illumination: YES
   - precalculated illumination: YES

 \section d12 Advantages
   - Compact representation. It requires only few bytes per vertex.
     Default implementation stores illumination as 3 floats, but you
     can easily add arbitrary compression.
   - Fast rendering.
     No sampler resources are consumed.
     It can be arbitrarily postprocessed in vertex shader.
     You may have for example multiple layers of precalculated indirect illumination 
     and mix them in vertex shader according to changes in scene.
   - There is no need to change your lightning equation,
     simply use our ambient data instead of constant ambient.
   - Ambient values don't depend on view angle, so rendering is very fast.

 \section d13 Disadvantages
   - Details are missing in areas without vertices.
     For good results, you have to add vertices to places where you miss details.
   - Seams around T vertices and other degenerated geometries.
     You have to make your meshes clean, avoid degeneracies.
   - Long narrow triangles (needles) often create visible artifacts.
     This is often problem also for physical engine,
     so your 3d artisis probably know they should avoid them.
   - Ambient values don't depend on view angle, so normal maps have no effect.

 \section d14 Data source
   - stored in: RRRealtimeRadiosity::getIllumination()->getChannel(0)->vertexBuffer
   - updated by: RRRealtimeRadiosity::calculate()

 \section d15 Data container
   - interface: RRIlluminationVertexBuffer
   - implementations: Platform independent vertex buffer is provided by 
     RRIlluminationVertexBuffer::createInSystemMemory().
     However, you are free to implement your own vertex buffer, see RRIlluminationVertexBuffer interface.
   - RRRealtimeRadiosity::newVertexBuffer uses
     RRIlluminationVertexBuffer::createInSystemMemory by default.

 \section d16 Data usage
   - Declare buffer as your second vertex stream and read ambient values from it
     in each vertex.



\page data_ambient_map Ambient map

 \section d21 Suitable for
   - static objects: YES
   - dynamic objects: NO
   - realtime calculated illumination: NO
   - precalculated illumination: YES

 \section d22 Advantages
   - High precision without additional vertices.
   - Very low resolution is sufficient (with good unwrap), compared to lightmaps.
     Ambient maps contain mostly low frequencies, no sharp edges.
   - Ambient values don't depend on view angle, so rendering is very fast.

 \section d23 Disadvantages
   - You should provide object unwrap.
   - Ambient values don't depend on view angle, so normal maps have no effect.

 \section d24 Data source
   - stored in: RRRealtimeRadiosity::getIllumination()->getChannel(0)->pixelBuffer
   - updated by: RRRealtimeRadiosity::calculate()

 \section d25 Data container
   - interface: RRIlluminationPixelBuffer
   - implementations: OpenGL pixel buffer is implemented in 
     RRIlluminationPixelBufferInOpenGL in DemoEngine,
     and you are free to use it.
     With commercial licence, source code is available in
     src/DemoEngine/RRIlluminationPixelBufferInOpenGL.cpp.
     For Direct3D you are expected to implement
     your own pixel buffer, see RRIlluminationPixelBuffer interface.
     We will provide Direct3D implementation in future version.
   - Default RRRealtimeRadiosity::newPixelBuffer returns NULL,
     so ambient maps are not generated by default.
     Return RRIlluminationPixelBufferInOpenGL or your implementation
     to start generating ambient maps.

 \section d26 Data usage
   - Map ambient texture to your object using your unwrap
     (you already had to provide unwrap to ambient map generator)
     and read ambient value from it in each pixel.
  


\page data_environment_map Environment map

 \section d31 Suitable for
   - static objects: YES
   - dynamic objects: YES
   - realtime calculated illumination: YES
   - precalculated illumination: YES

 \section d32 Advantages
   - offers global illumination with both specular and diffuse reflections

 \section d33 Disadvantages
   - precision decreases with size of object, suitable for items, not for buildings

 \section d34 Data source
   - stored in: your instances of RRIlluminationEnvironmentMap
   - updated by: RRRealtimeRadiosity::updateEnvironmentMap()

 \section d35 Data container
   - interface: RRIlluminationEnvironmentMap
   - implementations: OpenGL environment map is implemented in RRIlluminationEnvironmentMapInOpenGL
     in DemoEngine, and you are free to use it.
     For Direct3D you are expected to implement
     your own environment map, see RRIlluminationEnvironmentMap interface.
     OpenGL implementation is available in src/DemoEngine/RRIlluminationEnvironmentMapInOpenGL.cpp.
   - Note that environment maps may be implemented in many different ways.
     The most efficient one is a cube map, however you are free to implement your
     own format.

 \section d36 Data usage
   - Many rendering techniques are based on faked precomputed environment maps.
     Here you get realtime computed environment maps, and you are free to use them
     for any purpose.
   - Request environment (cube) map to be generated in center of your object.
     Environment map may be later used by GPU to add global illumination 
     to diffuse and specular surfaces close to given point in space.
   - We propose you image based lighting in pixel shader:
     \n
     For rough surface with mostly <b>diffuse</b> reflection,
     read value from environment map, small LOD (of size 2 or 4), coordinate 'surface normal'.
     This single instruction gives you 'diffuse' illumination of pixel.
     Multiply it by material color to get final color.
     \n
     For smooth surface with <b>specular</b> reflection,
     read value from environment map, big LOD (of size 8 or 16), coordinate 
     'eye direction reflected by surface'.
     These few instructions give you 'specular' illumination of pixel.
     Don't modulate it by material color, you already have final color.
     \n
     You can use <b>roughness map</b> to select per pixel which one
     of two techniques use or how to mix both together.
   - Global illumination can be further improved if you use <b>ambient occlusion map</b>
     for your dynamic object. Multiply global illumination readen from environment map
     by ambient occlusion readen from ambient occlusion map to get more precise result.
     Ambient occlusion maps are not generated by Lightsprint, but they are well integrated
     into many content creation tools, so there is good chance you can add them to
     your dynamic objects.
  


\page data_triangle Triangle or Vertex

 \section d41 Suitable for
   - static objects: YES
   - dynamic objects: NO
   - realtime calculated illumination: YES
   - precalculated illumination: YES
   - Illumination on few triangles is good for example for AI trying to find dark
     place for hiding.

 \section d42 Advantages
   - If you need information only for small subset of scene,
     querying single triangle is much faster
     than generating complete ambient map or vertex color buffer for whole object
     and reading value from it.
   - Even if you have ambient map or vertex buffer generated,
     it could be easier to use this query than searching
     individual value in vertex buffer/ambient map.

 \section d43 Disadvantages
   - Not demonstrated in samples yet, could require triangle number conversion.

 \section d44 Data source
   - RRScene::getTriangleMeasure with vertex=0,1,2 for triangle vertices
   - RRScene::getTriangleMeasure with vertex>2 for triangle surface
   - RRScene::getSubtriangleMeasure for adaptively subdivided triangles

 \section d45 Data container
   - RRColor for single illumination value
   - see \ref gunits

 \section d46 Data usage
   - Reading illumination level from triangle is slightly faster than reading
     it from vertex.



\page data_ray Ray

 \section d51 Suitable for
   - static objects: YES
   - dynamic objects: NO
   - realtime calculated illumination: YES
   - precalculated illumination: YES
   - Illumination at the ends of few rays is good for example for AI trying to find dark
     place for hiding.

 \section d52 Advantages
   - Fast access to single value.

 \section d53 Disadvantages
   - Not demonstrated in samples yet.

 \section d54 Data source
   - RRRealtimeRadiosity::getMultiObject()->getCollider()->intersect()
     to find static triangle at the end of ray
   - see \ref data_triangle for access to triangle's illumination

 \section d55 Data container
   - RRColor for single illumination value
   - see \ref gunits

 \section d56 Data usage




\page main_platforms Supported platforms

 \section plat_bin Platforms for binaries
 For binary libraries, supported platforms are
 - Win32 with Visual C++ 2005 multithreaded runtime library (use for example
   Microsoft's public <a href="http://www.microsoft.com/downloads/details.aspx?displaylang=en&FamilyID=32BC1BEE-A3F9-4C13-9C99-220B62A191EE">vcredist_x86.exe</a>
   to install it)
 - tested also under Windows XP x64, where it runs in 32bit
 - ask for more

 Supported CPUs are
 - x86 compatible with SSE
 - ask for more

 Supported GPUs are
 - all, no hard requirements
 - DX9 generation - standard target
 - DX10 generation - integration significantly simplified, performance improved

 Supported 3D APIs are
 - all, no hard requirements
 - OpenGL 2.0 - standard target with samples available
 - Direct3D 9 - standard target without samples
 - OpenGL 2.0 with EXT_geometry_shader - DX10 generation, optimized samples coming soon
 - Direct3D 10 - support coming soon

 Binaries should work with multiple compilers, but only these get our support:
 - Visual C++ 2005
 - ask for more

 \section plat_src Platforms for source code
 Our source code conforms to standard <b>ISO C++</b>, so you should be 
 able to use it on nearly any platform (consoles, linux etc).

 There are optional optimizations, that use SSE instructions on x86 CPUs,
 but they can be omitted on other platforms.



 
\page main_conventions Conventions

 \section gobjects Terminology
   Scene consists of triangle meshes and their instances with various
   positions, scales and materials - <b>objects</b>.
   Object that never moves, rotates, deforms or changes material properties is <b>static</b>.
   Other objects are <b>dynamic</b>.
 \section gunits Units (radiometry, photometry, screen)
   Lightsprint computes all in HDR.
   Whole documentation talks in radiometry terms like irradiance,
   and Lightsprint internally works in radiometry units.
   All inputs and nearly all outputs are 32bit float per component values.
   \n\n
   However, it is possible to communicate in screen colors
   or other units. Everything you need is to setup appropriate
   convertor, see RRScene::setScaler(). Scaler internally converts values from native
   physical radiometry scale to your custom scale and vice versa.
   \n\n
   In typical situations, it is most straightforward to think and communicate
   in screen colors. This means you can set nearly all inputs in screen colors
   (scaled to 0-1 range) and read all outputs in screen colors.
   To setup this mode, call RRRealtimeRadiosity::setScaler(RRScaler::createRgbScaler()).
   HelloRealtimeRadiosity demonstrates it.
 \section gscale Scale
   Lightsprint libraries support scaled objects.
   \n\n
   RRMesh and RRCollider support all scaled objects: positively or negatively, uniformly or non-uniformly scaled.
   \n\n
   RRObject, RRScene and RRRealtimeRadiosity support typical scaled objects:
   positively or negatively, uniformly scaled.
   \n Negative scale is supported with both possible interpretations
   for singlesided faces:
   Singlesided box visible from outside transformed with scale -1
   can stay visible form the outside or become visible only from inside,
   see RRObject::createWorldSpaceObject().
 \section gowner Ownership
   Dynamically created objects (using new) are never adopted, ownership never changes.
   \n This means that parameters that need to be destructed are never destructed inside call,
   responsibility for object is never passed to someone else.
   When you create object (using create() etc.), be sure that you delete it when
   no longer needed.
 \section gref Reference counting
   There is no internal reference counting, so if you create collider out of mesh,
   you are not allowed to destroy mesh before destroying collider. This danger should be
   mentioned on all appropriate places.
 \section gfinite Finite numbers
   If not otherwise specified, all inputs must be finite numbers.
   With Inf or NaN on input, result of any operation is undefined.
 \section gflodoub Floats and doubles
   Library uses both floats and doubles.
   It is not allowed to break double arithmetics by modifying FPU states.
   If you use Direct3D, make sure you don't instruct it to force single precision for whole application
   which breaks double precision arithmetics in whole program and libraries.
 \section gnull NULL
   Although NULL is obsoleted by C++ and some discourage from using it,
   we continue using it to distinguish zeros for pointers and zeros for non-pointers.
   So eg. if you see var=0, be sure that var is NOT a pointer. On the other side,
   var=NULL makes sure that var IS a pointer.
 \section gmatrices Matrices
   Lightsprint uses 3x4 matrices for description of object transformation.
   See RRMatrix3x4 for explanation why we found it optimal.






\page main_api API

 Lightsprint engine API is layered into following libraries:

 - <b> \subpage api_rr "REALTIME RADIOSITY" </b> - calculates realtime radiosity in dynamic scene

 - <b> \subpage api_illumination "ILLUMINATION" </b> - storage for calculated illumination

 - <b> \subpage api_vision "VISION" </b> - calculates radiosity in static scene

 - <b> \subpage api_collider "COLLIDER" </b> - finds ray-mesh intersections

 - <b> \subpage api_mesh "MESH" </b> - unifies access to triangle meshes

 and a standalone header that may turn into library in future:

 - <b> \subpage api_math "MATH" </b> - basic math

 Depending on your projects, you may use various subsets of whole API.

 See scheme of library dependencies:

 \image html libraries.png

 Lightsprint engine is purely numerical, platform independent.

 DemoEngine based on OpenGL 2.0 provides support for loading and rendering
 3d scenes with shaders, but without global illumination.
 It is not documented here, but its API is reasonably simple.
 You can safely replace DemoEngine by your renderer or game engine.

 Sample HelloRealtimeRadiosity uses both engines
 to render global illumination in scene with dynamic light and dynamic objects.
 It contains all glue classes needed for communication between
 Lightsprint engine and game engine.
 For use with other game engines, glue classes might need changes.

 Sample BunnyBenchmark measures Collider performance for comparison with
 physical engines (Collider is 2-200x faster).





\page api_rr Realtime Radiosity
 Realtime Radiosity extends your renderer by adding realtime computed
 indirect illumination.

 Headers: RRRealtimeRadiosity.h

 - adds indirect illumination to dynamic scenes
 - integrates with existing engines
 - uses no precalculations -> illumination quality varies, "architect edition"
 - techniques based on partial precalculations will follow -> quality boost, "day/night editions"
 - you can ask RealtimeRadiosity for complete vertex buffers, ambient maps
   or environment maps;
   for information on individual triangles (even adaptively subdivided) or rays,
   call underlying Vision library
 - communicates completely in custom units, e.g. screen colors

 Sample HelloRealtimeRadiosity shows the result of integration,
 interactive .3ds scene viewer with radiosity immediately responding
 to light movements.





\page api_illumination Illumination
 Illumination provides you with storage suitable for illumination
 calculated by RealtimeRadiosity.

 Headers: RRIllumination.h

 - illumination storage in ambient map: RRIlluminationPixelBuffer
 - illumination storage in vertex buffer: RRIlluminationVertexBuffer
 - illumination storage in environment map: RRIlluminationEnvironmentMap
 - storage of multiple illumination channels: RRObjectIllumination
 - allows for custom implementation -> smoothly integrates with other engines

 Sample HelloRealtimeRadiosity shows RRIllumination in action.




\page api_vision Vision
 In typical situation, you have your own renderer with direct illumination.
 Vision can enhance it by adding indirect illumination.

 In atypical sutuation, Vision can calculate global illumination
 without any relation to your renderer.

 Header: RRVision.h

 - calculates global illumination in static scene
 - progressive refinement with permanent access to results (you can start calculation and read results 1ms later, you will get raw approximation)
 - calculated illumination is available in vertices
 - communicates in physical or custom units (W/m^2 or e.g. screen colors)
 - display independent, purely numerical API
 - you can ask Vision about individual triangles, even adaptively subdivided;
   for complete vertex buffers or ambient maps, see RealtimeRadiosity library

 Sample HelloVision shows you the most simple use case:
 -# Create RRScene.
 -# Create RRObject using your object and insert it into scene. Repeat for all objects.
 -# Calculate global illumination using RRScene::illuminationImprove().
 -# Read results using RRScene::getTriangleMeasure().

 For integration with renderer, you may want to use some techniques from higher-level
 library Realtime Radiosity.



\page api_collider Collider
 Finds ray-mesh intersections.

 Header: RRCollider.h

 - thread safe, you can calculate any number of intersections at the same time
 - you can select technique in range from maximal speed to zero memory allocated
 - up to 2^32 vertices and 2^30 triangles in mesh
 - builds helper-structures and stores them in cache on disk

 Sample HelloCollider shows the most simple usage scenario:
 -# Create RRMesh using your vertex/index buffers.
 -# Create RRCollider using your mesh.
 -# Create RRRay using your ray.
 -# Call RRCollider::intersect() to find intersections. Repeat for all rays.

 Sample BunnyBenchmark shows how to detect collisions on all available
 CPUs/cores at once.


\page api_mesh Mesh
 Powerful interface to 3d triangle mesh.

 Header: RRMesh.h

 - knows tristrips, trilists, indexed or not (RRMesh::create, RRMesh::createIndexed)
 - can optimize:
   - vertex stitching (RRMesh::createOptimizedVertices)
   - removes degenerated triangles (RRMesh::createOptimizedTriangles)
 - merges many small meshes into one big mesh without additional memory (RRMesh::createMultiMesh)
 - saves/loads to disk (RRMesh::save, RRMesh::load)
 - extensible, you can add new channels like texture coords (RRChanneledData)
 - allows for procedural meshes, requires no memory (implementing your RRMesh takes few minutes)
 - up to 2^32-2 vertices and 2^32-2 triangles in mesh
 - thread safe, you can use mesh in any number of threads at the same time

 Sample HelloMesh shows the most simple usage scenario,
 mesh is created out of existing array of vertices.


\page api_math Math
 Basic math classes used by whole Lightsprint SDK.

 Header: RRMath.h

 - RRReal holds one real number, which is float at the moment
 - RRVec2 is vector of 2 real numbers
 - RRVec3 is vector of 3 real numbers
 - RRVec4 is vector of 4 real numbers

*/

};
