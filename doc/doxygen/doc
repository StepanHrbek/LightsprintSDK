namespace rr
{

/**


\mainpage Lightsprint SDK

 \image html intro.jpg

 Welcome to Lightsprint Software Development Kit.

 - <b> \subpage main_introduction </b>

 - <b> \subpage main_scenarios </b>

 - <b> \subpage main_integration </b>

 - <b> \subpage main_data_access </b>

 - <b> \subpage main_conventions "Conventions (units, scale, floats...)" </b>

 - <b> \subpage main_platforms </b>

 - <b> \subpage main_api "API reference" </b>




\page main_introduction Introduction

 \section intro1 Problem and Solution

  Realistic illumination simply looks good.
  While direct illumination has been 'solved' by science and software developers of 20th century,
  indirect illumination still challenges both sides. \subpage details

  Lightsprint is first to come with physically correct 
  indirect illumination synthesis so fast, that it 
  is suitable for realtime rendering in dynamic scenes.

 \section intro2 Your contribution

  Renderer with realtime direct illumination and shadows.

  If you don't have renderer, you can start with our renderer from DemoEngine.

  Example: Today's engines ignore indirect illumination from moving lights
  and render only direct illumination,
  or approximate indirect by constant ambient term.
  
  \image html a0b.jpg

  \image html all-ambient.png

 \section intro3 Our contribution

  Lightsprint calculates physically correct indirect illumination
  in realtime as a replacement for constant ambient.

  \image html a1b.jpg

  \image html all-rr.png

  We can also precalculate complete direct+indirect illumination from area lights.




\page details Details
 In our real world, we see visible light coming mostly from special surfaces 
 (hot wolfram fibre in bulb, luminofor of fluorescent lamp)
 or from whole volumes of plasma (sun, fire).
 We call these surfaces and volumes <b>source of direct illumination</b>.

 Light from sources of direct illumination reaches other surfaces 
 where part of light gets absorbed and part reflected.
 Reflected part is what we see and what makes objects look lit by direct illumination.
 When we see the reflected part, we say that object has <b>direct illumination</b>.

 Reflected light from direct illumination reaches other surfaces, partially reflects,
 reaches other surfaces, partially reflects etc.
 When we see sum of these reflected parts, we say that object has <b>indirect illumination</b>.

 Illumination we see in real world is sum of direct and indirect illumination.

 Computer graphics tries to simulate this process in order to generate realistic images.
 This is however very time consuming process.
 So realtime computer graphics in 99% of cases resigns to real-world sources of direct 
 illumination and uses imaginary <b>"point", "spot" or "directional" lights</b>.
 These fictitious sources of direct illumination
 allow realtime computer graphics to calculate direct illumination very quickly.
 This process includes calculation of <b>shadows in direct illumination</b>.
 There are many realtime techniques for calculating shadows in direct illumination,
 most notably shadow mapping, volumetric/stencil shadows and projected/texture based shadows.

 There is still problem with indirect illumination, which remains
 hard to be computed quickly.




\page main_scenarios Usage scenarios

 \section scenario2 Realtime GI in editor/tools. Game uses precalculated illumination

  - Saves months of work to graphics artists and designers,
    previously spent in lightmap rebuilds or fake light placement.

  - Allows artists to find visually more attractive positions / settings for lights.

  - Saves months spent in adapting your data for external global illumination
    tools. Other tools don't support arbitrary materials and lights.

  - Simple path for incorporating realtime radiosity into future games.

 \section scenario1 Realtime global illumination (GI) in game or architectural visualization

  - Greatly improves realism and visual appeal.

  - Requires no level preprocessing, saves time previously spent on infinite levels builds.

  - Simplifies integration of mods and other user provided assets.

  - Supports arbitrary materials / shaders, even those writen by modders
    after game release.

  - Publicity, be first with this level of realism never seen before.

  - See <a href="http://dee.cz/rrb/RRBugs.rar">Realtime Radiosity Bugs</a>
    as an example. It's not a real game, but it shows innovative use 
    of realtime radiosity in game. It doesn't have to be just eye candy.
    
 \image html rrbugs_hint.jpg





\page main_integration Adding GI to your code

Adding global illumination (GI) to your code is described in three steps.

 - <b> \subpage integration_milestone_1 </b>

 - <b> \subpage integration_milestone_2 </b>

 - <b> \subpage integration_milestone_3 </b>




\page integration_milestone_1 Milestone 1: GI in your scenes

 You can immediately render realtime global illumination in your 3d scenes
 if you convert them to .3ds format
 and load them into HelloRealtimeRadiosity sample.

 <table width=100% border=0 cellpadding=3 cellspacing=0><tr align=left style="background-color:#dddddd"><td>
 Recommended action:
 </td></tr></table>

  - If conversion tool exists,
    convert one of your scenes to .3ds format
    and load it into HelloRealtimeRadiosity.

 <table width=100% border=0 cellpadding=3 cellspacing=0><tr align=left style="background-color:#dddddd"><td>
 Scheme of application:
 </td></tr></table>

  - HelloRealtimeRadiosity application is
    built on top of purely numerical Lightsprint engine and OpenGL 2.0 based DemoEngine.
  \image html Integration1.png

 <table width=100% border=0 cellpadding=3 cellspacing=0><tr align=left style="background-color:#dddddd"><td>
 Steps:
 </td></tr></table>

  - <b> \subpage integration_step_1 </b> (necessary even if you don't convert to .3ds)
  - Convert textures to truecolor .tga.
  - Convert scenes to .3ds with 1m units and no transformations.
  - Change name of scene in HelloRealtimeRadiosity.cpp, so it loads your scene.

 <table width=100% border=0 cellpadding=3 cellspacing=0><tr align=left style="background-color:#dddddd"><td>
 Expected time spent:
 </td></tr></table>

  - 1 day

 <table width=100% border=0 cellpadding=3 cellspacing=0><tr align=left style="background-color:#dddddd"><td>
 Possibilities created:
 </td></tr></table>

  - Quickly write realtime GI demos based on your data.
  - Create level GI precalculator by adding save of computed illumination.



\page integration_milestone_2 Milestone 2: GI in your scene graph

  If you prefer other data source to .3ds files, next milestone is
  switch from .3ds files to your scene graph.

 <table width=100% border=0 cellpadding=3 cellspacing=0><tr align=left style="background-color:#dddddd"><td>
 Recommended action:
 </td></tr></table>

  - Modify HelloRealtimeRadiosity to render your scenes in their native format.

 <table width=100% border=0 cellpadding=3 cellspacing=0><tr align=left style="background-color:#dddddd"><td>
 Scheme of application:
 </td></tr></table>

  - Modified HelloRealtimeRadiosity newly depends on your scene loading code.
  \image html Integration2.png

 <table width=100% border=0 cellpadding=3 cellspacing=0><tr align=left style="background-color:#dddddd"><td>
 Steps:
 </td></tr></table>

  - Open HelloRealtimeRadiosity project
  - Copy 3DS2RR.* to Custom2RR.* and add it to the project
  - Delete \#include "DemoEngine/Model_3DS.h" from Custom2RR.h
  - Build reports errors on all references to 3ds,
    change code so it works with your engine's 3d model rather than with 3ds. For more details, see
    - comments in Custom2RR source code
    - <b> \subpage integration_step_2 </b>
    - <b> \subpage integration_step_3 </b>
    - interfaces of RRMesh and RRObject you implement
  - Go through HelloRealtimeRadiosity.cpp and
    - Replace all references to 3ds with your custom format.
    - Delete section "#ifndef AMBIENT_MAPS ... #endif", it relates only to 3ds.
  - If it works, you see your scene with two dynamic (still .3ds) objects.
    If you don't use 1m units, your scene could be too big, try scaling it.
  - Optional final step: do the same changes to DynamicObject,
    so it works with your format, rather than with .3ds.
 <table width=100% border=0 cellpadding=3 cellspacing=0><tr align=left style="background-color:#dddddd"><td>
 Expected time spent:
 </td></tr></table>

  - 1-2 days



\page integration_milestone_3 Milestone 3: GI in your renderer

  If you prefer other renderer to DemoEngine, final milestone is switch to renderer of your choice.

 <table width=100% border=0 cellpadding=3 cellspacing=0><tr align=left style="background-color:#dddddd"><td>
 Recommended action:
 </td></tr></table>

  - Modify HelloRealtimeRadiosity to use your renderer,
    or modify your application/engine to use Lightsprint engine.

 <table width=100% border=0 cellpadding=3 cellspacing=0><tr align=left style="background-color:#dddddd"><td>
 Scheme of application:
 </td></tr></table>

  - Different approaches: You can expose Lightsprint engine to your
    applications, hide it inside your engine, eventually
    your application is monolithic, without separated engine part.
  <table border=0 width=95%><tr align=top><td>
  \image html Integration3.png
  </td><td>
  \image html Integration4.png
  </td><td>
  \image html Integration5.png
  </td></tr></table>

 <table width=100% border=0 cellpadding=3 cellspacing=0><tr align=left style="background-color:#dddddd"><td>
 Steps:
 </td></tr></table>

  - Make sure your renderer supports realtime shadows other than stencil based, e.g. shadowmapping
    or projected shadows. If it doesn't, add them or contact us for further support.
    Commercial licence to Lightsprint engine contains full source code of DemoEngine,
    including realtime soft shadows.
  - <b> \subpage integration_step_4 </b>
  - <b> \subpage integration_step_5 </b>
  - Render our indirect illumination, see <b> \ref main_data_access </b>

 <table width=100% border=0 cellpadding=3 cellspacing=0><tr align=left style="background-color:#dddddd"><td>
 Expected time spent:
 </td></tr></table>

  - 1 week






\page integration_step_1 Validate static meshes

 This step is typically executed by 3d artists.

 Calculation produces better results
 when static 3d triangle meshes satisfy these conditions:

 \section cond_matching Cleanly connected triangles
 Two triangles are disjunct,
 share 1 vertex or share 1 edge and 2 vertices.

 So it is not allowed to
 - overlap triangles
 - intersect triangles
 - place edge in the middle of other triangle
 - place vertex in the middle of other edge

 \image html triangles.png

 See that all forbidden cases can be easily fixed.
 This operation can be automated.

 \section cond_no_needles No needles

 Angle sizes in triangles should be greater than 5 degrees.

 \section cond_no_hihpoly Prefer maps over high poly

 Polygon counts in dynamic objects have no influence on speed,
 but excessive polygon counts in static scene reduce performance.
 So for static scene,
 prefer normal maps or other tricks that reduce polygon count.






 
\page integration_step_2 Detect material properties

 Realtime radiosity is based on physically correct calculation of light transport.
 For such calculation it's necessary to know physical properties of surfaces.
 See RRSurface for list of all properties.

 Fortunately you don't have to study advanced laws of physics for good results,
 you don't even need any additional information provided by artists who
 create materials or compose shaders. Everything can be detected automatically.

 In <b>special case</b> of HelloRealtimeRadiosity example, materials contain diffuse texture
 without specular and transparency, so whole detection was reduced to calculating
 average color of diffuse texture and storing it into RRSurface::diffuseReflectance.
 This took 1 hour.

 In <b>more general case</b> with known set of shaders,
 you can extend this approach and get all values by analyzing textures and other inputs
 used by shaders.
 This can take you few hours.
 Viability of this approach highly depends on your shaders and can't be decided here.
 If you are not sure, describe us your shaders and we will help you.

 In <b>fully general case</b>, you don't need any information about shaders.
 It works even if you let modders create new shaders, their properties are detected.
 Everything you need is ability to render simple scene into small texture (16x16 pixels
 is typically enough)
 and calculate average color of rendered image. Follow these steps for each material

 -# Create empty scene and place rectangle covered by material in front of camera
    (so it covers whole viewport, but nothing more).
    If it has uv coordinates for textures, use whole texture space from 0,0 to 1,1 in rectangle.
 -# Clear to black and render rectangle.
    Store acquired average color as RRSurface::diffuseEmittance.
 -# Clear to white and render rectangle.
    Store acquired average color minus emittance as RRSurface::specularTransmittance.
 -# Add white light without distance attenuation to the same position as camera.
    Clear to black and render rectangle.
    Store acquired average color minus emittance as RRSurface::diffuseReflectance.
 -# Call RRSurface::validate() to clamp values to physically correct bounds.
    This is necessary to prevent damage from materials that e.g. reflect more light
    than they receive. Such behaviour is not possible in real world and may cause
    unexpected results in calculation such as infinitely high indirect illumination.

 This automatic approach can be further extended to differentiate between
 diffuse and specular reflectance or even to detect complete BRDF.

 Of course you are allowed to use any other approach, e.g. let graphics artists
 enter all values by hand.





\page integration_step_3 Create object wrappers

 Realtime radiosity transports light between object surfaces, so it needs to know everything
 about scene geometry and surface properties.

 Realtime radiosity is designed to access your structures in arbitrary format,
 so you don't have to duplicate any data. This may save you huge amounts of memory,
 however you must provide wrappers that access your structures.

 For <b>simple case</b>, see samples/HelloRealtimeRadiosity/3ds2rr.cpp. It is
 everything necessary to load .3ds model into realtime radiosity.
 For sake of simplicity, it duplicates some data and doesn't support geometry instancing.
 On the other hand, support for custom data is demonstrated
 on diffuse textures and uv coordinates.

 For <b>fully general</b> engine with geometry instancing, follow these steps:

 -  Create RRMesh for every static triangle mesh in your engine.
    \n\n
    You can immediately create them 
    from trilists, tristrips, indexed trilists, indexed tristrips
    using single call to RRMesh::create() or RRMesh::createIndexed().
    For other formats ask for our support or implement your own RRMesh,
    which is very simple.
    \n\n
    Note that these wrappers don't duplicate your data
    and they support mesh optimizations and mesh aggregation,
    see RRMesh for details.
    \n\n
    Once you have RRMesh instance, create RRCollider using
    RRCollider::create().
    RRCollider::IT_LINEAR technique with minimal overhead is sufficient 
    for current version of RRRealtimeRadiosity.
    \n\n
    Most convenient way to remember collider for later use is to
    attach it to your triangle mesh.
    You don't need to store pointer to RRMesh,
    it is available from collider using RRCollider::getMesh().
    \n\n
    RRRealtimeRadiosity may use collider to calculate ray-mesh collisions,
    but it is available also to you via RRCollider::intersect().
    If you plan to use it and its performance is critical, use 
    other technique than RRCollider::IT_LINEAR.

 -  Implement your own RRObject and
    create its instance for every static object in your scene.
    \n\n
    By objects we understand identical meshes placed on different
    positions in scene, possibly using different materials (e.g. several
    cars with the same geometry, different position and possibly different color;
    they are different objects sharing one mesh).
    \n\n
    Default RRObject::getTriangleMapping() returns realtime generated
    unwrap of low quality, for any serious use of ambient maps, you should override
    RRObject::getTriangleMapping() and provide your own unwrap.
    This is not necessary if you use vertex arrays and don't need ambient maps.
    \n\n
    Once you have RRObject instance, most convenient way to remember it
    for later use is to attach it to your object.





\page integration_step_4 Detect direct illumination in RRRealtimeRadiosity subclass

 RRRealtimeRadiosity contains several abstract methods, derive your class from RRRealtimeRadiosity
 and implement these methods.

 Check example implementation in HelloRealtimeRadiosity.
 You can see that the only nontrivial task was implementing 
 RRRealtimeRadiosity::detectDirectIllumination().
 
 <b>What needs to be detected:</b> average color of each face;
 how does it look lit by your direct (not ambient) point, spot and directional lights
 and shadowed by your shadows.
 This is typical approach, but interface is much more flexible,
 so you can detect any of face irradiance, incoming flux,
 exitance or exiting flux; and you can use any physical or non-physical scale.

 <b>Why is it important:</b> we have no other knowledge about your lights.
 You can use many light types with very complex lighting equations.
 You can arbitrarily change them. No problem. Just let us know what are the results -
 average colors produced by your shader.

 <b>How to implement it:</b> turn off ambient/radiosity, render all scene faces,
 read rendered image to system memory and extract average color for each face.
 This is the most simple and universal approach (works with any number of any lights),
 however you can think about alternatives. This depends highly on your renderer.

 \image html detect-dif.png
 Image shows arrangement of faces in matrix that makes extraction of average color/exitance simple.
 It wouldn't help to render faces in their original 3d positions, some would be probably
 hidden behind other faces. You can see that some faces are black, those are partially
 in shadow or completely unlit.
 You can also see that only 50% of texture space is used, triangles may be rearranged so that
 100% of space is used, however averaging face color would become more expensive.

 <b>Possible implementation in detail:</b> rendering faces into matrix
 requires one renderer enhancement - 2d position override - ability to render
 triangles to specified 2d positions while preserving their original look.

 For DX10 generation GPUs (GeForce 8xxx), solution is nearly as simple
 as adding two lines into geometry shader.
 Render as usual, using any combination of trilist/strip/indexed/nonindexed data,
 but at the end of geometry shader, override
 output vertex positions passed to rasterizer by new 2d triangle positions
 calculated right there from primitive id.
 \n You can use GeForce 8800 and multivendor OpenGL extension
 EXT_geometry_shader4 to write geometry shaders.
 New graphics cards and the same functionality in Direct3D 10 are expected soon.

 For DX9 generation GPUs, implemention has two steps:
 - Triangle positions in matrix are generated by CPU into new vertex stream.
   At the end of vertex shader, vertex position passed to fragment shader is replaced by
   position readen from additional vertex stream.
 - For purpose of detection, scene is rendered using non-indexed triangle list.
   This is necessary because if we want to render triangles with shared
   vertices to completely different positions in texture, we have to split 
   that vertices.
   In case of HelloRealtimeRadiosity, this is handled by branch in renderScene():
   after setting shader, two paths for specifying vertex data follow,
   - m3ds.Draw is original indexed tristrip path
   - rendererCaching->render() is new non-indexed trilist path 
     written only for purpose of this detection

 <i>
 <b>Optional optimizations:</b>

 <b>Rendering irradiance:</b>
 For simple materials with diffuse texture only (such as in .3ds),
 process can be optimized by ignoring diffuse textures, thus rendering incoming light
 not multiplied by diffuse texture.
 Detected average face colors then correspond to intensity of light reaching 
 face (irradiance) instead of intensity of light leaving face (exitance),
 so detected color is passed to RRObjectAdditionalIllumination::setTriangleAdditionalMeasure
 as RM_IRRADIANCE, rather than RM_EXITANCE.

 \image html detect-nodif.png
 Image above shows optimized detection, faces are not modulated by material,
 so irradiance is detected. You can see mostly white faces, because scene is lit by white spotlight.
 Few orange pixels come from orange logo projected by spotlight.

 Optimized approach is not suitable for engines with texture atlases.
 If your material properties change very significantly over uv space,
 use original unoptimized approach for higher precision.

 <b>GPU averaging:</b>
 To decrease amount of data transferred from GPU to CPU and speed up
 whole process, it is recommended to calculate average triangle colors
 on GPU, using simple shader, write them into smaller texture
 and transfer this smaller textue to CPU.
 It is demonstrated in HelloRealtimeRadiosity
 sample, using scaledown_filter.* shaders.

 \image html detect-scaled.png
 Image was scaled down by scaledown_filter.* shaders for purpose of
 faster primary illumination detection.
 It is resized back to original size only in this documentation.

 </i>




\page integration_step_5 Use RRRealtimeRadiosity subclass

 RRRealtimeRadiosity class is your primary interface to Lightsprint engine.
 To add global illumination to your application,

 - Create instance of your RRRealtimeRadiosity subclass.

 - To start calculation,
   call RRRealtimeRadiosity::setObjects() with set of static objects participating in calculation.
   This call is expensive, design your application to avoid
   frequent changes of static scene.

 - Call RRRealtimeRadiosity::calculate() often. If main loop of your
   application contains rendering of one frame, add one call to RRRealtimeRadiosity::calculate().
   If you render scene only when it has changed,
   still call RRRealtimeRadiosity::calculate() in every iteration of main loop,
   but if it returns true, rerender scene.

 - Call RRRealtimeRadiosity::reportLightChange whenever direct illumination changes.
   It is mainly when light moves or changes properties, but it is also when
   object moves and its shadow changes.

 - Call RRRealtimeRadiosity::reportMaterialChange whenever materials used in scene change.

 - Call RRRealtimeRadiosity::reportInteraction whenever user interacts or other reason for
   high responsiveness exists. Without reportInteraction calls, solver takes more CPU time
   and FPS decreases.

 - Call RRRealtimeRadiosity::getIllumination() 
   to acquire static object's indirect illumination, see \ref data_vertex_buffer and \ref data_ambient_map.

 - Call RRRealtimeRadiosity::updateEnvironmentMaps()
   to acquire static or dynamic object's indirect illumination, see \ref data_environment_map.

 Nearly all of these calls are demonstrated in HelloRealtimeRadiosity sample.






\page main_data_access Using computed GI
 Lightsprint engine calculates indirect illumination.
 To render with calculated illumination, you need access to calculated data structures.
 Lightsprint supports multiple data structures optimized for different use cases.

 Pick exactly one of these structures for each object
 and use it for rendering global illumination. You can use different structures
 for different objects.

 - <b> \subpage data_vertex_buffer </b>

 - <b> \subpage data_ambient_map </b>

 - <b> \subpage data_environment_map </b>

 For quick access to single value (for example by AI), you can query illumination for

 - <b> \subpage data_triangle </b>

 - <b> \subpage data_ray </b>



\page data_vertex_buffer Vertex color buffer

 \section d11 Suitable for
   - static objects: YES
   - dynamic objects: NO
   - realtime calculated illumination: YES
   - precalculated illumination: YES

 \section d12 Advantages
   - Compact representation. It requires only few bytes per vertex.
     Default implementation stores illumination as 3 floats, but you
     can easily add arbitrary compression.
   - Fast rendering.
     No sampler resources are consumed.
     It can be arbitrarily postprocessed in vertex shader.
     You may have for example multiple layers of precalculated indirect illumination 
     and mix them in vertex shader according to changes in scene.
   - There is no need to change your lightning equation,
     simply use our ambient data instead of constant ambient.
   - Ambient values don't depend on view angle, so rendering is very fast.

 \section d13 Disadvantages
   - Details are missing in areas without vertices.
     For good results, you have to add vertices to places where you miss details.
   - Seams around T vertices and other degenerated geometries.
     You have to make your meshes clean, avoid degeneracies.
   - Long narrow triangles (needles) often create visible artifacts.
     This is often problem also for physical engine,
     so your 3d artisis probably know they should avoid them.
   - Ambient values don't depend on view angle, so normal maps have no effect.

 \section d14 Data source
   - stored in: RRRealtimeRadiosity::getIllumination()->getChannel(0)->vertexBuffer
   - updated by: RRRealtimeRadiosity::calculate()

 \section d15 Data container
   - interface: RRIlluminationVertexBuffer
   - implementations: Platform independent vertex buffer is provided by 
     RRIlluminationVertexBuffer::createInSystemMemory().
     However, you are free to implement your own vertex buffer, see RRIlluminationVertexBuffer interface.
   - RRRealtimeRadiosity::newVertexBuffer uses
     RRIlluminationVertexBuffer::createInSystemMemory by default.

 \section d16 Data usage
   - Stream data from vertex buffer into vertex shader, interpret them
     in shader as an ambient light value.

 \section d17 Examples
   - OpenGL example:
     \n Rendering with per vertex ambient.
     \code
	rr::RRRealtimeRadiosity* realtimeRadiosity;
	GLuint program;
	...
	// set program created from shaders below
	glUseProgram(program);
	// get vertex buffer with indirect illumination
	rr::RRIlluminationVertexBuffer* vertexBuffer = realtimeRadiosity->
		getIllumination(numberOfObject)->getChannel(0)->vertexBuffer;
	// enable stream with color values
	glEnableClientState(GL_COLOR_ARRAY);
	// set pointer to color data for first vertex
	glColorPointer(3, GL_FLOAT, 0, &vertexBuffer->lock()->x);
	// render primitives
	glDrawElements...
	// cleanup
	vertexBuffer->unlock();
	glDisableClientState(GL_COLOR_ARRAY);
     \endcode
     Using ambient value in GLSL vertex shader:
     \code
	varying vec3 ambientLight;
	void vertexShader()
	{
		...
		ambientLight = gl_Color;
	}
     \endcode
     Using ambient value in GLSL fragment shader:
     \code
	varying vec3 ambientLight;
	void fragmentShader()
	{
		...
		gl_FragColor = ... + materialColor * vec4(ambientLight.xyz,0.0);
	}
     \endcode
   - Direct3D 9 example:
     \n Rendering with per vertex ambient.
     \code
	IDirect3DDevice9* device;
	IDirect3DPixelShader9* vertexShader;
	IDirect3DPixelShader9* pixelShader;
	// adapt your vertex declaration, let your mesh read data from stream 0
	// and add e.g. COLOR1 readen from stream 1
	IDirect3DVertexDeclaration9* vertexDeclaration;
	device->CreateVertexDeclaration(description, &vertexDeclaration);
	...
	// create d3d vertex buffer and fill it with vertexBuffer->lock() data
	rr::RRIlluminationVertexBuffer* vertexBuffer = realtimeRadiosity->
		getIllumination(numberOfObject)->getChannel(0)->vertexBuffer;
	IDirect3DVertexBuffer9* d3dBuffer = ...;
	// to prevent data duplication and copying, implement RRIlluminationVertexBuffer
	//  that stores data directly into d3d vertex buffer
	...
	// set rendering pipeline to use shaders below
	device->SetPixelShader(vertexShader);
	device->SetPixelShader(pixelShader);
	// activate previously created vertex declaration
	device->SetVertexDeclaration(vertexDeclaration);
	// set pointer to your mesh (vertices, possibly normals etc.) in stream 0
	device->SetStreamSource(0, ...);
	// set pointer to vertex illumination data in stream 1
	device->SetStreamSource(1, d3dBuffer, ...);
	// render primitives
	device->DrawPrimitive...
	// cleanup
	device->SetStreamSource(1, NULL, 0, 0);
	device->SetStreamSource(0, NULL, 0, 0);
     \endcode
     Using ambient value in HLSL vertex shader:
     \code
	void vertexShader(in float3 iAmbientLight: COLOR1,
		..., out float3 oAmbientLight: COLOR1)
	{
		...
		oAmbientLight = iAmbientLight;
	}
     \endcode
     Using ambient value in HLSL pixel shader:
     \code
	void pixelShader(in float3 iAmbientLight: COLOR1,
		..., out float4 oColor: COLOR)
	{
		...
		oColor = ... + materialColor * float4(iAmbientLight,0);
	}
     \endcode
   - Alternatively, applying colors from vertex buffer could be done in fixed pipeline,
     without shaders, but it is beyond scope of this documentation.
   - See Direct3D, OpenGL or your engine documentation for more details
     on streaming per vertex data to vertex shader and rendering with ambient light.



\page data_ambient_map Ambient map

 \section d21 Suitable for
   - static objects: YES
   - dynamic objects: NO
   - realtime calculated illumination: NO
   - precalculated illumination: YES

 \section d22 Advantages
   - High precision without additional vertices.
   - Very low resolution is sufficient (with good unwrap), compared to lightmaps.
     Ambient maps contain mostly low frequencies, no sharp edges.
   - Ambient values don't depend on view angle, so rendering is very fast.

 \section d23 Disadvantages
   - You need additional uv channel with object's unwrap (for ambient map mapping).
     If you don't have it, ask your 3d artists
     to bake unwrap into meshes as an additional uv channel.
     Unwraps are often genrated automatically, using existing free or commercial tools.
   - Ambient values don't depend on view angle, so normal maps have no effect.

 \section d24 Data source
   - stored in: RRRealtimeRadiosity::getIllumination()->getChannel(0)->pixelBuffer
   - updated by: RRRealtimeRadiosity::calculate()

 \section d25 Data container
   - interface: RRIlluminationPixelBuffer
   - implementations: OpenGL pixel buffer is implemented in 
     RRIlluminationPixelBufferInOpenGL,
     source code is part of HelloRealtimeRadiosity sample.
     For Direct3D, you are expected to implement
     your own pixel buffer, see RRIlluminationPixelBuffer interface.
   - Default RRRealtimeRadiosity::newPixelBuffer returns NULL,
     so ambient maps are not generated by default.
     Return RRIlluminationPixelBufferInOpenGL or your implementation
     to start generating ambient maps.

 \section d26 Data usage
   - Map ambient texture to your object using your uv channel with object's unwrap,
     read ambient value from texture in pixel shader
     and interpret it as an ambient light.

 \section d27 Examples
   - Generic example:
     \n Providing access to unwrap in your implementation of rr::RRObject interface.
     \code
	// access to uv channel with object's unwrap
	void YourImplementationOfRRObject::getTriangleMapping(
		unsigned t, TriangleMapping& out) const
	{
		for(unsigned v=0;v<2;v++)
		{
			// copy uv baked with your mesh
			// for vertex v (v=0..2) in triangle t
			out.uv[v][0] = ...; // u coordinate
			out.uv[v][1] = ...; // v coordinate
		}
	}
     \endcode
   - OpenGL example:
     \n Creating ambient map.
     \code
	virtual rr::RRIlluminationPixelBuffer*
	YourImplementationOfRRRealtimeRadiosity::newPixelBuffer(rr::RRObject* object)
	{
		// use rr::RRIlluminationPixelBufferInOpenGL from HelloRealtimeRadiosity
		// or your implementation
		return new rr::RRIlluminationPixelBufferInOpenGL(256,256,"shaders/");
	}
     \endcode
     Rendering with ambient map.
     \code
	rr::RRRealtimeRadiosity* realtimeRadiosity;
	GLuint program;
	...
	// set program created from shaders below
	glUseProgram(program);
	// bind ambient map to texture0
	glActiveTexture(GL_TEXTURE0);
	realtimeRadiosity->getIllumination(numberOfObject)->
		getChannel(0)->pixelBuffer->bindTexture();
	// set sampler to use texture0
	glUniform1i(glGetUniformLocation(program,"ambientMap"),0);
	// enable stream with texture coordinates
	glEnableClientState(GL_TEXTURE_COORD_ARRAY);
	// set pointer to texture coordinates
	glColorPointer(2, GL_FLOAT, 0, array with uv values of unwrap);
	// render primitives
	glDrawElements...
	// cleanup
	glDisableClientState(GL_TEXTURE_COORD_ARRAY);
     \endcode
     Using uv coordinates in GLSL vertex shader:
     \code
	varying vec2 ambientMapCoord;
	void vertexShader()
	{
		...
		ambientMapCoord = gl_TexCoord.xy;
	}
     \endcode
     Sampling and using ambient value in GLSL fragment shader:
     \code
	uniform sampler2D ambientMap;
	varying vec2 ambientMapCoord;
	void fragmentShader()
	{
		vec4 ambientLight = texture2D(ambientMap, ambientMapCoord);
		...
		gl_FragColor = ... + materialColor * ambientLight;
	}
     \endcode
   - Direct3D 9 example:
     \n Creating ambient map.
     \code
	virtual rr::RRIlluminationPixelBuffer*
	YourImplementationOfRRRealtimeRadiosity::newPixelBuffer(rr::RRObject* object)
	{
		// use your implementation of rr::RRIlluminationPixelBuffer
		return new YourImplementationOfRRIlluminationPixelBuffer(...);
	}
     \endcode
     Rendering with ambient map.
     \code
	IDirect3DDevice9* device;
	IDirect3DPixelShader9* vertexShader;
	IDirect3DPixelShader9* pixelShader;
	// create vertex declaration that includes ambient map uv channel as TEXCOORD0
	IDirect3DVertexDeclaration9* vertexDeclaration;
	device->CreateVertexDeclaration(description, &vertexDeclaration);
	rr::RRRealtimeRadiosity* realtimeRadiosity;
	...
	// set rendering pipeline to use shaders below
	device->SetPixelShader(vertexShader);
	device->SetPixelShader(pixelShader);
	// set sampler to use ambient map
	//  (implement pixelBuffer class in d3d)
	realtimeRadiosity->getIllumination(numberOfObject)->
		getChannel(0)->pixelBuffer->bindTexture();
	// set vertex declaration for your mesh data,
	//  including uv channel with unwrap
	device->SetVertexDeclaration(vertexDeclaration);
	// set pointer to your mesh in stream 0
	//  (vertices, uv channel, possibly normals etc.)
	device->SetStreamSource(0, ...);
	// render primitives
	device->DrawPrimitive...
	// cleanup
	device->SetStreamSource(0, NULL, 0, 0);
     \endcode
     Using uv coordinates in HLSL vertex shader:
     \code
	void vertexShader(in float2 iAmbientMapCoord: TEXCOORD0,
		..., out float2 oAmbientMapCoord: TEXCOORD0)
	{
		...
		oAmbientLight = iAmbientLight;
	}
     \endcode
     Sampling and using ambient value in HLSL pixel shader:
     \code
	sampler ambientMap;
	void pixelShader(in float2 iAmbientMapCoord: TEXCOORD0,
		..., out float4 oColor: COLOR)
	{
		float4 ambientLight = tex2D(ambientMap, iAmbientMapCoord);
		...
		oColor = ... + materialColor * ambientLight;
	}
     \endcode
   - Alternatively, texturing could be done in fixed pipeline,
     without shaders, but it is beyond scope of this documentation.
   - See Direct3D, OpenGL or your engine documentation for more details
     on texturing and rendering with ambient light.
  


\page data_environment_map Environment map

 \section d31 Suitable for
   - static objects: YES
   - dynamic objects: YES
   - realtime calculated illumination: YES
   - precalculated illumination: YES

 \section d32 Advantages
   - offers global illumination with both specular and diffuse reflections

 \section d33 Disadvantages
   - precision decreases with size of object, suitable for characters and items, not for buildings

 \section d34 Data source
   - stored in: your instances of RRIlluminationEnvironmentMap
   - updated by: RRRealtimeRadiosity::updateEnvironmentMaps()

 \section d35 Data container
   - interface: RRIlluminationEnvironmentMap
   - implementations: OpenGL environment map is implemented in RRIlluminationEnvironmentMapInOpenGL,
     source code is part of HelloRealtimeRadiosity sample.
     For Direct3D, you are expected to implement
     your own environment map, see RRIlluminationEnvironmentMap interface.

 \section d36 Data usage
   - Many rendering techniques are based on faked precomputed environment maps.
     Here you get realtime computed environment maps, and you are free to use them
     for any purpose.
   - Request environment (cube) map to be generated in center of your object.
     Environment map may be later used by GPU to add global illumination 
     to diffuse and specular surfaces close to given point in space.
   - We propose you several techniques:
     \n
     For rough surface with mostly <b>diffuse</b> reflection,
     read value from diffuse environment map, using 'surface normal' as a coordinate.
     This single instruction gives you global illumination of pixel.
     Multiply it by material diffuse color to get final color.
     Size 4 of environment map is sufficient for close objects and 2 for distant ones.
     \n
     For smooth surface with <b>specular</b> reflection,
     read value from specular environment map, using
     'eye direction reflected by surface' as a coordinate.
     These few instructions give you global illumination of pixel.
     Don't modulate it by material color unless you want to render
     exotic materials, you already have final color.
     Size 16 of environment map simulates smooth surfaces, size 4 simulates rough
     surface.
     \n
     You can use <b>specular map</b> to select per pixel which one
     of two techniques to use or how to mix both together.
     \n
     You can use <b>normal map</b> to modulate surface normal.
     Both diffuse and specular surfaces respond well to normal maps.
     \n
     DemoEngine implements all of these techniques.
   - Global illumination can be further improved if you use <b>ambient occlusion map</b>
     for your dynamic object. Multiply global illumination readen from environment map
     by ambient occlusion readen from ambient occlusion map to get more precise result.
     Ambient occlusion maps are not generated by Lightsprint, but they are well integrated
     into many content creation tools, so there is good chance you can add them to
     your dynamic objects.

 \section d37 Examples
   - OpenGL example:
     \n Rendering with environment maps.
     \code
	rr::RRRealtimeRadiosity* realtimeRadiosity;
	rr::RRVec3 center; // object's center position in world
	rr::RRIlluminationEnvironmentMap* diffuseEnvironmentMap;
	rr::RRIlluminationEnvironmentMap* specularEnvironmentMap;
	GLuint program;
	...
	// update environment maps
	realtimeRadiosity->updateEnvironmentMaps(center,16,
		16,specularEnvironmentMap,4,diffuseEnvironmentMap);
	// set program created from shader below
	glUseProgram(program);
	// bind diffuse environment map to texture0
	glActiveTexture(GL_TEXTURE0);
	diffuseEnvironmentMap->bindTexture(); // calls glBindTexture(GL_TEXTURE_2D,map);
	// set sampler to use texture0
	glUniform1i(glGetUniformLocation(program,"diffuseEnvironmentMap"),0);
	// bind specular environment map to texture1
	glActiveTexture(GL_TEXTURE1);
	specularEnvironmentMap->bindTexture(); // calls glBindTexture(GL_TEXTURE_2D,map);
	// set sampler to use texture1
	glUniform1i(glGetUniformLocation(program,"specularEnvironmentMap"),1);
	// render primitives
	glDrawElements...
     \endcode
     Applying environment maps in GLSL fragment shader:
     \code
	uniform samplerCube specularEnvironmentMap;
	uniform samplerCube diffuseEnvironmentMap;
	void fragmentShader()
	{
		// normal in world space, you may apply normal map here
		vec3 worldNormal = ...;
		// view vector in world space = position of fragment - position of camera
		vec3 worldView = ...;
		// reflected view vector in world space
		vec3 worldViewReflected = reflect(worldView,worldNormal);
		...
		gl_FragColor = ...
			// diffuse reflection
			+ materialDiffuseReflectance *
			  textureCube(diffuseEnvironmentMap, worldNormal)
			// specular reflection
			+ materialSpecularReflectance *
			  textureCube(specularEnvironmentMap, worldViewReflected);
	}
     \endcode
   - Direct3D 9 example:
     \n Rendering with environment maps.
     \code
	IDirect3DDevice9* device;
	IDirect3DPixelShader9* pixelShader;
	rr::RRRealtimeRadiosity* realtimeRadiosity;
	rr::RRVec3 center; // object's center position in world
	rr::RRIlluminationEnvironmentMap* diffuseEnvironmentMap;
	rr::RRIlluminationEnvironmentMap* specularEnvironmentMap;
	...
	// update environment maps
	realtimeRadiosity->updateEnvironmentMaps(center,16,
		16,specularEnvironmentMap,4,diffuseEnvironmentMap);
	// set rendering pipeline to use shader below
	device->SetPixelShader(pixelShader);
	// set samplers to use environment maps
	// (implement environment map class in d3d)
	diffuseEnvironmentMap->bindTexture();
	specularEnvironmentMap->bindTexture();
	// render primitives
	device->DrawPrimitive...
     \endcode
     Applying environment maps in HLSL pixel shader:
     \code
	samplerCUBE specularEnvironmentMap;
	samplerCUBE diffuseEnvironmentMap;
	void pixelShader(..., out float4 oColor: COLOR)
	{
		// normal in world space, you may apply normal map here
		float3 worldNormal = ...;
		// view vector in world space = position of fragment - position of camera
		float3 worldView = ...;
		// reflected view vector in world space
		float3 worldViewReflected = reflect(worldView,worldNormal);
		...
		oColor = ...
			// diffuse reflection
			+ materialDiffuseReflectance *
			  texCUBE(diffuseEnvironmentMap, worldNormal)
			// specular reflection
			+ materialSpecularReflectance *
			  texCUBE(specularEnvironmentMap, worldViewReflected);
	}
     \endcode
   - See Direct3D, OpenGL or your engine documentation for more details
     on texturemapping and applying environment maps.



\page data_triangle Triangle or Vertex

 \section d41 Suitable for
   - static objects: YES
   - dynamic objects: NO
   - realtime calculated illumination: YES
   - precalculated illumination: YES
   - Illumination on few triangles is good for example for AI trying to find dark
     place for hiding.

 \section d42 Advantages
   - If you need information only for small subset of scene,
     querying single triangle is much faster
     than generating complete ambient map or vertex color buffer for whole object
     and reading value from it.
   - Even if you have ambient map or vertex buffer generated,
     it could be easier to use this query than searching
     individual value in vertex buffer/ambient map.

 \section d43 Disadvantages
   - Not demonstrated in samples yet, could require triangle number conversion.

 \section d44 Data source
   - RRScene::getTriangleMeasure with vertex=0,1,2 for triangle vertices
   - RRScene::getTriangleMeasure with vertex>2 for triangle surface
   - RRScene::getSubtriangleMeasure for adaptively subdivided triangles

 \section d45 Data container
   - RRColor for single illumination value
   - see \ref gunits

 \section d46 Data usage
   - Reading illumination level from triangle is slightly faster than reading
     it from vertex.



\page data_ray Ray

 \section d51 Suitable for
   - static objects: YES
   - dynamic objects: NO
   - realtime calculated illumination: YES
   - precalculated illumination: YES
   - Illumination at the ends of few rays is good for example for AI trying to find dark
     place for hiding.

 \section d52 Advantages
   - Fast access to single value.

 \section d53 Disadvantages
   - Not demonstrated in samples yet.

 \section d54 Data source
   - RRRealtimeRadiosity::getMultiObject()->getCollider()->intersect()
     to find static triangle at the end of ray
   - see \ref data_triangle for access to triangle's illumination

 \section d55 Data container
   - RRColor for single illumination value
   - see \ref gunits

 \section d56 Data usage




\page main_platforms Supported platforms

 \section plat_bin Platforms for binaries
 For binary libraries, supported platforms are
 - Win32 with Visual C++ 2005 multithreaded runtime library (use for example
   Microsoft's public <a href="http://www.microsoft.com/downloads/details.aspx?displaylang=en&FamilyID=32BC1BEE-A3F9-4C13-9C99-220B62A191EE">vcredist_x86.exe</a>
   to install it)
 - tested also under Windows XP x64, where it runs in 32bit
 - ask for more

 Supported CPUs are
 - x86 compatible with SSE
 - ask for more

 Supported GPUs are
 - all, no hard requirements
 - DX9 generation - standard target
 - DX10 generation - integration significantly simplified, performance improved
 - soft shadows in DemoEngine require at least GeForce 6xxx or Radeon 9500,
   but they are not mandatory

 Supported 3D APIs are
 - all, no hard requirements
 - OpenGL 2.0 - standard target with samples available
 - Direct3D 9 - standard target without samples
 - OpenGL 2.0 with EXT_geometry_shader - DX10 generation, optimized samples planned
 - Direct3D 10 - support planned

 Supported library configurations are
 - Release DLL
 - others go with commercial licence,
   please run examples in Release DLL during evaluation period

 Binaries should work with multiple compilers, but only these get our support:
 - Visual C++ 2005
 - ask for more

 \section plat_src Platforms for source code
 Our source code conforms to standard <b>ISO C++</b>, so you should be 
 able to use it on nearly any platform (consoles, linux etc).

 There are optional optimizations, that use SSE instructions on x86 CPUs,
 but they can be omitted on other platforms.



 
\page main_conventions Conventions

 \section gobjects Terminology
   Scene consists of triangle meshes and their instances with various
   positions, scales and materials - <b>objects</b>.
   Object that never moves, rotates, deforms or changes material properties is <b>static</b>.
   Other objects are <b>dynamic</b>.
 \section gunits Units (radiometry, photometry, screen)
   Lightsprint computes all in HDR.
   Whole documentation talks in radiometry terms like irradiance,
   and Lightsprint internally works in radiometry units.
   All illumination measure inputs and outputs are 32bit float per component values.
   \n\n
   However, it is possible to communicate in screen colors
   or other units. Everything you need is to setup appropriate
   convertor, see RRRealtimeRadiosity::setScaler(). Scaler internally converts values from native
   physical radiometry scale to your custom scale and vice versa.
   \n\n
   In typical situations, it is most straightforward to think and communicate
   in screen colors. This means you can set nearly all inputs in screen colors
   (scaled to 0-1 range) and read all outputs in screen colors.
   To setup this mode, call RRRealtimeRadiosity::setScaler(RRScaler::createRgbScaler()).
   HelloRealtimeRadiosity demonstrates it.
 \section gscale Scale
   Lightsprint libraries support scaled objects.
   \n\n
   RRMesh and RRCollider support all scaled objects: positively or negatively, uniformly or non-uniformly scaled.
   \n\n
   RRObject, RRScene and RRRealtimeRadiosity support typical scaled objects:
   positively or negatively, uniformly scaled.
   \n Negative scale is supported with both possible interpretations
   for singlesided faces:
   Singlesided box visible from outside transformed with scale -1
   can stay visible form the outside or become visible only from inside,
   see RRObject::createWorldSpaceObject().
 \section gowner Ownership
   Dynamically created objects (using new) are never adopted, ownership never changes.
   \n This means that parameters that need to be destructed are never destructed inside call,
   responsibility for object is never passed to someone else.
   When you create object (using create() etc.), always delete it when it's
   no longer needed.
 \section gref Reference counting
   There is no internal reference counting, so if you create collider out of mesh,
   you are not allowed to destroy mesh before destroying collider. This danger should be
   mentioned on all appropriate places.
 \section gfinite Finite numbers
   If not otherwise specified, all inputs must be finite numbers.
   With Inf or NaN on input, result of any operation is undefined.
 \section gflodoub Floats and doubles
   Library uses both floats and doubles.
   It is not allowed to break double arithmetics by modifying FPU states.
   If you use Direct3D, make sure you don't instruct it to force single precision for whole application
   which breaks double precision arithmetics in whole program and libraries.
 \section gnull NULL
   Although NULL is obsoleted by C++ and some discourage from using it,
   we continue using it to distinguish zeros for pointers from zeros for non-pointers.
   So if you see x=0, x is NOT a pointer.
   If you see x=NULL, x IS a pointer.
 \section gmatrices Matrices
   Lightsprint uses 3x4 matrices for description of object transformation.
   See RRMatrix3x4 for explanation why we found it optimal.






\page main_api API

 Lightsprint engine API is layered into following libraries:

 - <b> \subpage api_rr "REALTIME RADIOSITY" </b> - calculates realtime radiosity in dynamic scene

 - <b> \subpage api_illumination "ILLUMINATION" </b> - storage for calculated illumination

 - <b> \subpage api_vision "VISION" </b> - calculates radiosity in static scene

 - <b> \subpage api_collider "COLLIDER" </b> - finds ray-mesh intersections

 - <b> \subpage api_mesh "MESH" </b> - unifies access to triangle meshes

 and standalone header that may turn into library in future:

 - <b> \subpage api_math "MATH" </b> - basic math

 Outside Lightsprint engine, SDK contains:

 - <b> \subpage api_demoengine "DEMOENGINE" </b> - OpenGL 2.0 engine/renderer with soft shadows

 Depending on your projects, you may use various subsets of whole API.

 See scheme of library and sample dependencies:

 \image html libraries.png

 Lightsprint engine is purely numerical, platform independent.

 DemoEngine based on OpenGL 2.0 provides support for loading and rendering
 3d scenes with shaders, but without global illumination.
 You can safely replace DemoEngine by your renderer or game engine.

 Sample HelloRealtimeRadiosity uses both engines
 to render global illumination in scene with dynamic light and dynamic objects.
 It contains all glue classes needed for communication between
 Lightsprint engine and game engine.
 For use with other game engines, glue classes might need changes.

 Sample BunnyBenchmark measures Collider performance for comparison with
 physical engines (Collider is 2-200x faster).





\page api_rr Realtime Radiosity
 Realtime Radiosity extends your renderer by adding realtime computed
 global illumination.

 Headers: RRRealtimeRadiosity.h

 - adds global illumination to dynamic scenes
 - integrates with existing engines
 - uses no precalculations -> illumination quality varies, "architect edition"
 - techniques based on partial precalculations will follow -> quality boost, "day/night editions"
 - you can ask RealtimeRadiosity for complete vertex buffers, ambient maps
   or environment maps;
   for information on individual triangles (even adaptively subdivided) or rays,
   call underlying Vision library
 - communicates completely in custom units, e.g. screen colors

 Sample HelloRealtimeRadiosity shows the result of integration,
 interactive .3ds scene viewer with global illumination immediately responding
 to object and light movements.





\page api_illumination Illumination
 Illumination provides you with storage suitable for illumination
 calculated by RealtimeRadiosity.

 Headers: RRIllumination.h

 - illumination storage in ambient map: RRIlluminationPixelBuffer
 - illumination storage in vertex buffer: RRIlluminationVertexBuffer
 - illumination storage in environment map: RRIlluminationEnvironmentMap
 - storage of multiple illumination channels: RRObjectIllumination
 - allows for custom implementation -> smoothly integrates with other engines

 Sample HelloRealtimeRadiosity shows RRIllumination in action.




\page api_vision Vision
 In typical situation, you have your own renderer with direct illumination.
 Vision can enhance it by adding indirect illumination.

 In atypical sutuation, Vision can calculate global illumination
 without any relation to your renderer.

 Header: RRVision.h

 - calculates global illumination in static scene
 - progressive refinement with permanent access to results (you can start calculation and read results 1ms later, you will get raw approximation)
 - calculated illumination is available in vertices
 - communicates in physical or custom units (W/m^2 or e.g. screen colors)
 - display independent, purely numerical API
 - you can ask Vision about individual triangles, even adaptively subdivided;
   for complete vertex buffers or ambient maps, see RealtimeRadiosity library

 Sample HelloVision shows you the most simple use case:
 -# Create RRScene.
 -# Create RRObject using your object and insert it into scene. Repeat for all objects.
 -# Calculate global illumination using RRScene::illuminationImprove().
 -# Read results using RRScene::getTriangleMeasure().

 For integration with renderer, you may want to use some techniques from higher-level
 library Realtime Radiosity.



\page api_collider Collider
 Finds ray-mesh intersections.

 Header: RRCollider.h

 - thread safe, you can calculate any number of intersections at the same time
 - you can select technique in range from maximal speed to zero memory allocated
 - up to 2^32 vertices and 2^30 triangles in mesh
 - builds helper-structures and stores them in cache on disk

 Sample HelloCollider shows the most simple usage scenario:
 -# Create RRMesh using your vertex/index buffers.
 -# Create RRCollider using your mesh.
 -# Create RRRay using your ray.
 -# Call RRCollider::intersect() to find intersections. Repeat for all rays.

 Sample BunnyBenchmark shows how to detect collisions on all available
 CPUs/cores at once.


\page api_mesh Mesh
 Powerful interface to 3d triangle mesh.

 Header: RRMesh.h

 - knows tristrips, trilists, indexed or not (RRMesh::create, RRMesh::createIndexed)
 - can optimize:
   - vertex stitching (RRMesh::createOptimizedVertices)
   - removes degenerated triangles (RRMesh::createOptimizedTriangles)
 - merges many small meshes into one big mesh without additional memory (RRMesh::createMultiMesh)
 - saves/loads to disk (RRMesh::save, RRMesh::load)
 - extensible, you can add new channels like texture coords (RRChanneledData)
 - allows for procedural meshes, requires no memory (implementing your RRMesh takes few minutes)
 - up to 2^32-2 vertices and 2^32-2 triangles in mesh
 - thread safe, you can use mesh in any number of threads at the same time

 Sample HelloMesh shows the most simple usage scenario,
 mesh is created out of existing array of vertices.


\page api_math Math
 Basic math classes used by whole Lightsprint SDK.

 Header: RRMath.h

 - RRReal holds one real number, which is float at the moment
 - RRVec2 is vector of 2 real numbers
 - RRVec3 is vector of 3 real numbers
 - RRVec4 is vector of 4 real numbers



\page api_demoengine DemoEngine
 DemoEngine based on OpenGL 2.0 provides support for loading and rendering
 3d scenes with shaders, but without global illumination.
 You can safely replace DemoEngine by your renderer or game engine.

 Headers: DemoEngine/*.h

 - de::Texture is OpenGL texture
 - de::Program is GLSL program
 - de::UberProgram is GLSL program with preprocessor parameters changeable at runtime
 - de::UberProgramSetup is set of parameters for our ubershaders UberShader.vp and UberShader.fp
 - de::Camera is frustum suitable for camera or spotlight
 - de::AreaLight is area light
 - de::Model_3DS is model loaded from .3ds file
 - de::Renderer is generic renderer interface

 Features and limitations of DemoEngine
   - for Windows (request support for other platforms)
   - uses OpenGL 2.0
   - supports all OpenGL 2.0 cards, but soft shadows require GeForce 6xxx or higher, Radeon 9500 or higher
   - spotlights with realtime shadows
   - linear and area spotlights (multiple overlapping spotlights)
   - no omnidirectional lights with realtime shadows
   - scenes and objects loaded from .3ds
   - separately enabled/disabled light features:
     - color
     - projected texture
     - shadows with variable softness, resolution
     - diffuse environment map with variable resolution
     - specular environment map with variable resolution
   - separately enabled/disabled material features:
     - diffuse color per vertex
     - diffuse color per pixel
     - specular (enabled for whole object)
     - specular map (simple, specular modulated by diffuse map)
     - normal map (simple, normal modulated by diffuse map)

 Sample HelloDemoEngine shows .3ds scene viewer with dynamic objects
 and realtime soft shadows, based on DemoEngine.

*/

};
