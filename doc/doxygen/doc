namespace rr
{

/**


\mainpage Lightsprint SDK
 Welcome to Lightsprint SDK.

 \subpage rr_dir_indir

 \subpage scenarios

 \subpage rr_integration "Integration (with editor, game)"

 \subpage vision_storage "Data formats (vertex arrays, ambient maps)"

 \subpage rules "Conventions (units, scale, floats...)"

 \subpage platforms

 \subpage api "API reference"

 \image html all-rr.png




\page rr_dir_indir Direct vs indirect; Realtime Radiosity position
 In our real world, we see visible light coming mostly from special surfaces 
 (hot wolfram fibre in bulb, luminofor of fluorescent lamp)
 or from whole volumes of plasma (sun, fire).
 We call these surfaces and volumes <b>source of direct illumination</b>.

 Light from sources of direct illumination reaches other surfaces 
 where part of light gets absorbed and part reflected.
 Reflected part is what we see and what makes objects look lit by direct illumination.
 When we see the reflected part, we say that object has <b>direct illumination</b>.

 Reflected light from direct illumination reaches other surfaces, partially reflects,
 reaches other surfaces, partially reflects etc.
 When we see sum of these reflected parts, we say that object has <b>indirect illumination</b>.

 Illumination we see in real world is sum of direct and indirect illumination.

 Computer graphics tries to simulate this process in order to generate realistic images.
 This is however very time consuming process.
 So realtime computer graphics in 99% of cases resigns to real-world sources of direct 
 illumination and uses imaginary <b>"point", "spot" or "directional" lights</b>.
 These fictitious sources of direct illumination
 allow realtime computer graphics to calculate direct illumination very quickly.
 This process includes calculation of <b>shadows in direct illumination</b>.
 There are many realtime techniques for calculating shadows in direct illumination,
 most notably shadow mapping, volumetric/stencil shadows and projected/texture based shadows.

 There is however still problem with indirect illumination, which remains
 hard to compute quickly.

 You are expected to <b>have renderer with direct illumination and shadows</b> in 
 direct illumination. This is very modest expectation, as there are
 many realtime renderers having it, both commercial and open source.
 (If you still don't have your own renderer, you can start with our renderer from DemoEngine.)

 <b>RealtimeRadiosity adds indirect illumination</b> into such renderer.

 Vision is also able to calculate complete illumination (direct+indirect)
 from real-world direct light sources - surfaces. However we don't prefer
 this use case as it's not suitable for interactive application.

 \image html all-ambient.png
 Today's games with moving lights approximate indirect illumination
 by constant ambient. Rarely, they render no indirect illumination
 at all (left column).

 \image html all-rr.png
 Lightsprint calculates physically correct indirect illumination
 (realtime radiosity) as a replacement for constant ambient.




\page scenarios Usage scenarios

 \section scenario2 Realtime radiosity integrated into editor. Game uses precalculated lightmaps / vertex arrays

  - Saves months of work to graphics artists and designers,
    previously spent in lightmap rebuilds or fake light placement.

  - Allows artists to find visually more attractive positions / settings for lights.

  - Saves months spent in adapting your data for external global illumination
    tools. Other tools don't support arbitrary materials and lights.

  - Simple path for incorporating realtime radiosity into future games.

 \section scenario1 Realtime radiosity integrated into game

  - Greatly improves realism and visual appeal.

  - Requires no level preprocessing, saves time previously spent on infinite levels builds.

  - Simplifies integration of mods and other user provided assets.

  - Supports arbitrary materials / shaders, even those writen by modders
    after game release.

  - Publicity, be first with realtime global illumination in game.

  - See <a href="http://dee.cz/rrb/RRBugs.rar">Realtime Radiosity Bugs</a>
    as an example. It's not a real game, but it shows innovative use 
    of realtime radiosity in game. It doesn't have to be just eye candy.
    
 \image html rrbugs_hint.jpg





\page rr_integration Integration

 You can quickly start playing with your data
 if you convert them to .3ds and load into HelloRealtimeRadiosity.
 (Use 1m units, truecolor .tga textures and no transformations.)

 To integrate realtime radiosity into your editor, game or any other application,
 follow these steps:

 -# \subpage integration_step_1
 -# \subpage integration_step_2
 -# Stop rendering constant ambient
 -# Start rendering ambient from vertex stream or ambient map provided by RRRealtimeRadiosity::getIllumination()->getChannel(0)
 -# \subpage integration_step_5
 -# \subpage integration_step_6

 Concept behind realtime radiosity integration is also described in
 <a href="http://dee.cz/rri">Realtime Radiosity Integration article</a>.

 Final result of integration is demonstrated in HelloRealtimeRadiosity example.






\page integration_step_1 Detect material properties

 Realtime radiosity is based on physically correct calculation of light transport.
 For such calculation it's necessary to know physical properties of surfaces.
 See RRSurface for list of all properties.

 Fortunately you don't have to study advanced laws of physics for good results,
 you don't even need any additional information provided by artists who
 create materials or compose shaders. Everything can be detected automatically.

 In <b>special case</b> of HelloRealtimeRadiosity example, materials contain diffuse texture
 without specular and transparency, so whole detection was reduced to calculating
 average color of diffuse texture and storing it into RRSurface::diffuseReflectance.
 This took 1 hour.

 In <b>more general case</b> with known set of shaders,
 you can extend this approach and get all values by analyzing textures and other inputs
 used by shaders.
 This can take you few hours.
 Viability of this approach highly depends on your shaders and can't be decided here.
 If you are not sure, describe us your shaders and we will help you.

 In <b>fully general case</b>, you don't need any information about shaders.
 It will work even if you let modders create new shaders, their properties will be detected.
 Everything you need is ability to render simple scene into small texture (16x16 pixels
 is typically enough)
 and calculate average color of rendered image. Follow these steps for each material

 -# Create empty scene and place rectangle covered by material in front of camera
    (so it covers whole viewport, but nothing more).
    If it has uv coordinates for textures, use whole texture space from 0,0 to 1,1 in rectangle.
 -# Clear to black and render rectangle.
    Store acquired average color as RRSurface::diffuseEmittance.
 -# Clear to white and render rectangle.
    Store acquired average color minus emittance as RRSurface::specularTransmittance.
 -# Add white light without distance attenuation to the same position as camera.
    Clear to black and render rectangle.
    Store acquired average color minus emittance as RRSurface::diffuseReflectance.
 -# Call RRSurface::validate() to clamp values to physically correct bounds.
    This is necessary to prevent damage from materials that eg. reflect more light
    than they receive. Such behaviour is not possible in real world and may cause
    unexpected results in calculation such as infinitely high indirect illumination.

 This automatic approach can be further extended to differentiate between
 diffuse and specular reflectance or even to detect complete BRDF.

 Of course you are allowed to use any other approach, eg. let graphics artists
 enter all values by hand.





\page integration_step_2 Create object wrappers

 Realtime radiosity transports light between object surfaces, so it needs to know everything
 about scene geometry and surface properties.

 Realtime radiosity is designed to access your structures in arbitrary format,
 so you don't have to duplicate any data. This may save you huge amounts of memory,
 however you must provide wrappers that access your structures.

 For <b>simple case</b>, see src/DemoEngine/3ds2rr.cpp. It is source code
 of everything necessary to load .3ds model into realtime radiosity.
 It is simple because .3ds doesn't support geometry instancing.
 On the other hand, support for custom data is demonstrated
 on diffuse textures and uv coordinates.
 For sake of simplicity, it duplicates some data.

 For <b>fully general</b> engine with geometry instancing, follow these steps:

 -  Create RRMesh for every triangle mesh in your engine.
    \n\n
    You can immediately create them 
    from trilists, tristrips, indexed trilists, indexed tristrips
    using single call to RRMesh::create() or RRMesh::createIndexed().
    For other formats ask for our support or implement your own RRMesh,
    which is very simple.
    \n\n
    Note that these wrappers don't duplicate your data
    and they support mesh optimizations and mesh aggregation,
    see RRMesh for details.
    \n\n
    Once you have RRMesh instance, create RRCollider using
    RRCollider::create().
    RRCollider::IT_LINEAR technique with minimal overhead is sufficient 
    for current version of RealtimeRadiosity.
    \n\n
    Most convenient way to remember collider for later use is to
    attach it to your triangle mesh.
    You don't need to store pointer to RRMesh,
    it is available from collider using RRCollider::getMesh().
    \n\n
    Realtime radiosity may use collider to calculate ray-mesh collisions,
    but it is available also to you via RRCollider::intersect().
    If you plan to use it and its performance is critical, use 
    other technique than RRCollider::IT_LINEAR.

 -  Implement your own RRObject and
    create its instance for every object in your scene.
    \n\n
    By objects we understand identical meshes placed on different
    positions in scene, possibly using different materials (eg. several
    cars with the same geometry, different position and possibly different color;
    they are different objects sharing one mesh).
    \n\n
    Once you have RRObject instance, most convenient way to remember it
    for later use is to attach it to your object.





\page integration_step_5 Subclass RRRealtimeRadiosity

 RRRealtimeRadiosity contains several abstract methods, derive your class from RRRealtimeRadiosity
 and implement these methods.

 Check example implementation in HelloRealtimeRadiosity.
 You can see that the only nontrivial task was implementing 
 RRRealtimeRadiosity::detectDirectIllumination().
 
 <b>What needs to be detected:</b> simply said, average color of each face;
 how does it looks lit by your direct (not ambient) point, spot and directional lights
 and shadowed by your shadows.
 It is optional if you detect face irradiance, incoming flux,
 exitance or exiting flux.

 <b>Why is it important:</b> we have no other knowledge about your lights.
 You can use many light types with very complex lighting equations.
 You can arbitrarily change them. No problem. Just let us know what are the results -
 average colors produced by your shader.

 <b>How to implement it:</b> turn off ambient/radiosity, render all scene faces,
 read rendered image to system memory and extract average color for each face.
 This is the most simple and universal approach (works with any number of any lights),
 however you can think about alternatives. This depends highly on your renderer.

 \image html detect-dif.png
 Image shows arrangement of faces in matrix that makes extraction of average color/exitance simple.
 It wouldn't help to render faces in their original 3d positions, some would be probably
 hidden behind other faces. You can see that some faces are black, those are partially
 in shadow or completely unlit.
 You can also see that only 50% of texture space is used, triangles may be rearranged so that
 100% of space is used, however averaging face color would become more expensive.

 <b>Possible implementation in detail:</b> rendering faces into matrix
 requires one renderer enhancement - 2d position override - ability to render
 triangles to specified 2d positions while preserving their original look.

 For current generation GPUs, implemention has two steps:
 - Triangle positions in matrix are generated by CPU into new vertex stream.
   At the end of vertex shader, vertex position passed to fragment shader is replaced by
   position readen from additional vertex stream.
 - For purpose of detection, scene is rendered using non-indexed triangle list.
   This is necessary because if we want to render triangles with shared
   vertices to completely different positions in texture, we have to split 
   that vertices.
   In case of HelloRealtimeRadiosity, this is handled by branch in renderScene():
   after setting shader, two paths for specifying vertex data follow,
   - m3ds.Draw is original indexed tristrip path
   - rendererCaching->render() is new non-indexed trilist path 
     written only for purpose of this detection

 <i>
 <b>Optional reading:</b>
 for next generation GPUs with "primitive index", solution is uncomparably easier.
 Render as usual, using any combination of trilist/strip/indexed/nonindexed data,
 but at the end of vertex shader, override
 vertex position passed to fragment shader by new 2d triangle position
 calculated right there from primitive index.

 <b>Faster detection:</b>
 for simple materials with diffuse texture only (such as in .3ds)
 process can be optimized by ignoring diffuse textures, thus rendering incoming light
 not multiplied by diffuse texture.
 Detected average face colors then correspond to intensity of light reaching 
 face (irradiance) instead of intensity of light leaving face (exitance),
 so detected color is passed to RRObjectAdditionalIllumination::setTriangleAdditionalMeasure with RM_IRRADIANCE
 instead of RM_EXITANCE.

 \image html detect-nodif.png
 Image above shows optimized detection, faces are not modulated by material
 so irradiance is detected. You can see mostly white faces, because scene is lit by white spotlight.
 Few orange pixels come from orange logo projected by spotlight.

 Optimized approach is not suitable for engines with texture atlases.
 If your material properties change very significantly over uv space,
 use original unoptimized approach for higher precision.
 </i>





\page integration_step_6 Use RRRealtimeRadiosity subclass

 To finish integration with your application (editor or game),
 create instance of your RRRealtimeRadiosity subclass
 and call its several methods on appropriate places of your application
 as described here:

 To start radiosity calculation, call RRRealtimeRadiosity::setObjects()
 with set of objects participating in calculation.

 Call RRRealtimeRadiosity::calculate() often. If main loop of your
 application contains rendering of one frame, add one call to RRRealtimeRadiosity::calculate().
 If you render scene only when it has changed,
 still call RRRealtimeRadiosity::calculate() in every iteration of main loop,
 but if it returns true, take it as signal that scene has changed.

 Use "reportSomething" methods to report events, see RRRealtimeRadiosity for details.

 Use RRRealtimeRadiosity::getIllumination() to acquire object's indirect illumination during render.

 See example in samples/HelloRealtimeRadiosity/HelloRealtimeRadiosity.cpp.





\page vision_storage Data formats
 Illumination levels are calculated for all surfaces in your scene.
 Multiple ways how to access these levels and how to store them exist.

 \section vertices Vertices, Vertex arrays
   You can read illumination levels in individual face vertices
   (RRScene::getTriangleMeasure)
   or let RealtimeRadiosity generate complete vertex arrays for you
   (RRRealtimeRadiosity::calculate).
   It is expected that illumination inside face will be linearly interpolated
   from values in vertices.
   \n This representation has several advantages:
   - Compact representation. It requires only few bytes per vertex.
     Default implementation stores illumination as 3 floats, but you
     can easily add arbitrary compression.
   - Fast rendering.
     No sampler resources are consumed.
     It can be arbitrarily postprocessed in vertex shader.
     You may have for example multiple layers of precalculated indirect illumination 
     and mix them in vertex shader according to changes in scene.

   On the other hand, there are also disadvantages of vertex illumination:
   - Details are missing in areas without vertices.
     For good results, you have to add vertices to places where you miss details.
   - Seams around T vertices and other degenerated geometries.
     You have to make your meshes clean, avoid degeneracies.
   - Long narrow triangles (needles) often create visible artifacts.
     This is often problem also for physical engine,
     so your 3d artisis probably know they should avoid them.

   Vertex arrays are stored by RRIlluminationVertexBuffer
   interface.
   Implementation is provided 
   (RRIlluminationVertexBuffer::createInSystemMemory)
   and it is used by default (by RRRealtimeRadiosity::newVertexBuffer).

 \section textures Ambient maps
   You can read illumination levels in adaptively subdivided triangles
   (RRScene::getSubtriangleMeasure)
   and create ambient maps out of them or let Realtime Radiosity
   do it for you (RRRealtimeRadiosity::calculate).
   This representation is more expensive for processing, storage and rendering
   (any postprocessing must be done per pixel and it consumes sampler resources),
   but it avoids some disadvantages of vertex illumination
   (needle triangles cause problems to both vertex arrays and ambient maps).

   At the moment, adaptive subdivision is turned off by default, 
   and ambient maps contain no additional details compared to vertex arrays.
   This will be fixed soon. API won't change, you'll just get more 
   detailed ambient maps.

   Ambient maps are stored by RRIlluminationPixelBuffer
   interface.
   No implementation is present in platform independent core libraries
   (default RRRealtimeRadiosity::newPixelBuffer returns NULL),
   so ambient maps are not generated by default.
   However you can use OpenGL accelerated implementation from DemoEngine
   (RRIlluminationPixelBufferInOpenGL)
   or implement your own.

 \section faces Faces
   You can read illumination levels in individual faces
   (RRScene::getTriangleMeasure with vertex=3).
   This is good for example for AI trying to find dark place for hiding.
   Reading illumination level on face is slightly faster than reading
   it in vertex.
   


\page platforms Supported platforms

 \section plat_bin Platforms for binaries
 For binary libraries, supported platforms are
 - Win32 with Visual C++ 2005 multithreaded runtime library (use for example
   Microsoft's public <a href="http://www.microsoft.com/downloads/details.aspx?displaylang=en&FamilyID=32BC1BEE-A3F9-4C13-9C99-220B62A191EE">vcredist_x86.exe</a>
   to install it)
 - tested also under Windows XP x64, where it runs in 32bit
 - ask for more

 Supported CPUs are
 - x86 compatible with SSE
 - ask for more

 Binaries may work with multiple compilers, but only these are tested:
 - Visual C++ 2005
 - Visual C++ 2003

 \section plat_src Platforms for source code
 Our source code conforms to standard <b>ISO C++</b>, so you should be 
 able to use it on nearly any platform (consoles, linux etc).

 There are optional optimizations, that use SSE instructions on x86 CPUs,
 but they can be omitted on other platforms.


 
\page rules Conventions

 \section gunits Units (radiometry, photometry, screen)
   Although whole documentation talks in radiometry terms,
   it is possible to work in photometry units, screen colors 
   or any other units.
   In typical situations, it is most straightforward to work and think 
   in screen colors. Everything you need is to setup appropriate
   convertor, see RRScene::setScaler().

 \section gscale Scale
   Lightsprint libraries support scaled objects.
   \n\n
   RRMesh and Collider support all scaled objects: positively or negatively, uniformly or non-uniformly scaled.
   \n\n
   RRObject and Vision support typical scaled objects: positively or negatively, uniformly scaled.
   \n Negative scale is supported with both possible interpretations
   for singlesided faces:
   Singlesided box visible from outside transformed with scale -1
   can stay visible form the outside or become visible only from inside,
   see RRObject::createWorldSpaceObject().
 \section gowner Ownership
   Dynamically created objects (using new) are never adopted, ownership never changes.
   \n This means that parameters that need to be destructed are never destructed inside call,
   responsibility for object is never passed to someone else.
   When you create object (using create() etc.), be sure that you delete it when
   no longer needed.
 \section gref Reference counting
   There is no internal reference counting, so if you create collider out of mesh,
   you are not allowed to destroy mesh before destroying collider. This danger should be
   mentioned on all appropriate places.
 \section gfinite Finite numbers
   If not otherwise specified, all inputs must be finite numbers.
   With Inf or NaN on input, result of any operation is undefined.
 \section gflodoub Floats and doubles
   Library uses both floats and doubles.
   It is not allowed to break double arithmetics by modifying FPU states.
   If you use Direct3D, make sure you don't instruct it to force single precision for whole application
   which breaks double precision arithmetics in whole program and libraries.
 \section gnull NULL
   Although NULL is obsoleted by C++ and some discourage from using it,
   we continue using it to distinguish zeros for pointers and zeros for non-pointers.
   So eg. if you see var=0, be sure that var is NOT a pointer. On the other side,
   var=NULL makes sure that var IS a pointer.






\page api API

 \subpage rr "REALTIME RADIOSITY"
 - library, calculates realtime radiosity in dynamic scene

 \subpage illumination "ILLUMINATION"
 - library, storage for calculated illumination

 \subpage vision "VISION"
 - library, calculates radiosity in static scene

 \subpage collider "COLLIDER"
 - library, finds ray-mesh intersections

 \subpage mesh "MESH"
 - library, unifies access to triangle meshes

 \subpage math "MATH"
 - header, basic math

 Lightsprint libraries are simply layered.
 Each layer solves new unique problem using lower layers.
 Depending on your project, you can use all layers or just one.

 Mesh provides unified interface and manipulation for all 3d triangle meshes.
 \n Collider calculates ray x mesh intersections and depends on Mesh.
 \n Vision calculates radiosity in static scene and depends on Collider.
 \n Illumination manages calculated illumination in ambient maps or vertex buffers.
 \n RealtimeRadiosity calculates radiosity in dynamic scene and depends on Vision and Illumination.

 All core libraries (white boxes in scheme, RealtimeRadiosity, Illumination,
 Vision, Collider, Mesh) are purely numerical,
 they are OpenGL/DirectX independent.

 DemoEngine provides simple support for loading and rendering 3d scenes.
 It depends on OpenGL, GLU and RealtimeRadiosity. It has built-in GLEW.

 Sample HelloRealtimeRadiosity uses RealtimeRadiosity, OpenGL and GLUT
 for demonstrating realtime radiosity in scene with dynamic light.
 It uses DemoEngine for common tasks that can be used later by other samples.

 \image html libraries.png

 Other samples depend only on appropriate libraries, eg. HelloCollider on Collider.

 BunnyBenchmark measures Collider performance for comparison with other engines.





\page rr Realtime Radiosity
 Realtime Radiosity extends your renderer by adding realtime computed
 indirect illumination.

 Headers: RRRealtimeRadiosity.h

 - adds indirect illumination to dynamic scenes
 - integrates with existing engines
 - uses no precalculations -> illumination quality varies, "architect edition"
 - techniques based on partial precalculations will follow -> quality boost, "day/night editions"
 - you can ask RealtimeRadiosity for complete vertex buffers or ambient maps;
   for information on individual triangles (even adaptively subdivided), call underlying Vision library

 Sample HelloRealtimeRadiosity shows the result of integration,
 interactive .3ds scene viewer with radiosity immediately responding
 to light movements.





\page illumination Illumination
 Illumination provides you with storage suitable for illumination
 calculated by RealtimeRadiosity.

 Headers: RRIllumination.h

 - illumination storage in ambient map: RRIlluminationPixelBuffer
 - illumination storage in vertex buffer: RRIlluminationVertexBuffer
 - storage of multiple illumination channels: RRObjectIllumination
 - allows for custom implementation -> smoothly integrates with other engines

 Sample HelloRealtimeRadiosity shows RRIllumination in action.




\page vision Vision
 In typical situation, you have your own renderer with direct illumination.
 Vision can enhance it by adding indirect illumination.

 In atypical sutuation, Vision can calculate global illumination
 without any relation to your renderer.

 Header: RRVision.h

 - calculates global illumination in static scene
 - progressive refinement with permanent access to results (you can start calculation and read results 1ms later, you will get raw approximation)
 - calculated illumination is available in vertices
 - works with your units (screen colors or radiometry or photometry units or anything else)
 - display independent, purely numerical API
 - you can ask Vision about individual triangles, even adaptively subdivided;
   for complete vertex buffers or ambient maps, see RealtimeRadiosity library

 Sample HelloVision shows you the most simple use case:
 -# Create RRScene.
 -# Create RRObject using your object and insert it into scene. Repeat for all objects.
 -# Calculate global illumination using RRScene::illuminationImprove().
 -# Read results using RRScene::getTriangleMeasure().

 For integration with renderer, you may want to use some techniques from higher-level
 library Realtime Radiosity.



\page collider Collider
 Finds ray-mesh intersections.

 Header: RRCollider.h

 - thread safe, you can calculate any number of intersections at the same time
 - you can select technique in range from maximal speed to zero memory allocated
 - up to 2^32 vertices and 2^30 triangles in mesh
 - builds helper-structures and stores them in cache on disk

 Sample HelloCollider shows the most simple usage scenario:
 -# Create RRMesh using your vertex/index buffers.
 -# Create RRCollider using your mesh.
 -# Create RRRay using your ray.
 -# Call RRCollider::intersect() to find intersections. Repeat for all rays.

 Sample BunnyBenchmark shows how to detect collisions on all available
 CPUs/cores at once.


\page mesh Mesh
 Powerful interface to 3d triangle mesh.

 Header: RRMesh.h

 - knows tristrips, trilists, indexed or not (RRMesh::create, RRMesh::createIndexed)
 - can optimize:
   - vertex stitching (RRMesh::createOptimizedVertices)
   - removes degenerated triangles (RRMesh::createOptimizedTriangles)
 - merges many small meshes into one big mesh without additional memory (RRMesh::createMultiMesh)
 - saves/loads to disk (RRMesh::save, RRMesh::load)
 - extensible, you can add new channels like texture coords (RRChanneledData)
 - allows for procedural meshes, requires no memory (implementing your RRMesh takes few minutes)
 - up to 2^32-2 vertices and 2^32-2 triangles in mesh
 - thread safe, you can use mesh in any number of threads at the same time

 Sample HelloMesh shows the most simple usage scenario,
 mesh is created out of existing array of vertices.


\page math Math
 Basic math classes used by whole Lightsprint SDK.

 Header: RRMath.h

 - RRReal holds one real number, which is float at the moment
 - RRVec2 is vector of 2 real numbers
 - RRVec3 is vector of 3 real numbers
 - RRVec4 is vector of 4 real numbers

*/

};
